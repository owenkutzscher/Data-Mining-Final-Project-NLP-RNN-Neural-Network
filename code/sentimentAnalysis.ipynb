{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "# After doing some research, we will want to use TensorFlow, prob Keras (a deep\n",
    "# learning API written on top of TensorFlow, it's currently being used\n",
    "# in the LHC (Large Hadron Collider)).\n",
    "\n",
    "# We will be classifying text with BERT:\n",
    "# https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
    "\n",
    "# NOTE: as of 10/19/23 tensorflow will not run on windowns\n",
    "# I recomend running this through jupyterlab on a linux kernal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-text==2.13.* in /Applications/anaconda3/lib/python3.8/site-packages (2.13.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-text==2.13.*) (0.15.0)\n",
      "Requirement already satisfied: tensorflow<2.14,>=2.13.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-text==2.13.*) (2.13.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.12.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.20.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.13.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.34.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.6.3)\n",
      "Requirement already satisfied: packaging in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (23.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.3.0)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (23.5.26)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.23.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (4.5.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.59.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.13.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (52.0.0.post20210125)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Applications/anaconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.36.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.7.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.23.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.25.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Applications/anaconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Applications/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Applications/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "# A dependency of the preprocessing for BERT inputs\n",
    "!pip install -U \"tensorflow-text==2.13.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-models-official==2.13.* in /Applications/anaconda3/lib/python3.8/site-packages (2.13.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.10.1)\n",
      "Requirement already satisfied: oauth2client in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (4.1.3)\n",
      "Requirement already satisfied: immutabledict in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (3.0.0)\n",
      "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (0.7.5)\n",
      "Requirement already satisfied: tensorflow~=2.13.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.13.1)\n",
      "Requirement already satisfied: gin-config in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (0.5.0)\n",
      "Requirement already satisfied: sentencepiece in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (0.1.99)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (0.15.0)\n",
      "Requirement already satisfied: tensorflow-text~=2.13.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.13.0)\n",
      "Requirement already satisfied: Cython in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (0.29.23)\n",
      "Requirement already satisfied: matplotlib in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.23.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (6.0.1)\n",
      "Requirement already satisfied: seqeval in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.2.2)\n",
      "Requirement already satisfied: Pillow in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (8.2.0)\n",
      "Requirement already satisfied: pycocotools in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.0.7)\n",
      "Requirement already satisfied: py-cpuinfo>=3.3.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (9.0.0)\n",
      "Requirement already satisfied: sacrebleu in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.3.1)\n",
      "Requirement already satisfied: opencv-python-headless in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (4.8.1.78)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.2.4)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (5.8.0)\n",
      "Requirement already satisfied: tensorflow-datasets in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (4.9.2)\n",
      "Requirement already satisfied: tf-slim>=1.1.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.1.0)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.104.0)\n",
      "Requirement already satisfied: kaggle>=1.3.9 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.5.16)\n",
      "Requirement already satisfied: six in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.15.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.12.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (0.1.1)\n",
      "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.23.3)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (4.1.1)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (0.22.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.25.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (1.61.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (3.20.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (0.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Applications/anaconda3/lib/python3.8/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.4.7)\n",
      "Requirement already satisfied: certifi in /Applications/anaconda3/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (2020.12.5)\n",
      "Requirement already satisfied: python-slugify in /Applications/anaconda3/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (8.0.1)\n",
      "Requirement already satisfied: bleach in /Applications/anaconda3/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (3.3.0)\n",
      "Requirement already satisfied: urllib3 in /Applications/anaconda3/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (1.26.4)\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (4.59.0)\n",
      "Requirement already satisfied: python-dateutil in /Applications/anaconda3/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Applications/anaconda3/lib/python3.8/site-packages (from pandas>=0.22.0->tf-models-official==2.13.*) (2021.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Applications/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (0.5.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.10)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.59.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.12.1)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (23.5.26)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.2.0)\n",
      "Requirement already satisfied: setuptools in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (52.0.0.post20210125)\n",
      "Requirement already satisfied: packaging in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (23.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.4.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (4.5.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.6.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (16.0.6)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.34.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Applications/anaconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow~=2.13.0->tf-models-official==2.13.*) (0.36.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (1.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Applications/anaconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Applications/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.4.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.2.2)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.13.*) (0.1.8)\n",
      "Requirement already satisfied: webencodings in /Applications/anaconda3/lib/python3.8/site-packages (from bleach->kaggle>=1.3.9->tf-models-official==2.13.*) (0.5.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->tf-models-official==2.13.*) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->tf-models-official==2.13.*) (1.3.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /Applications/anaconda3/lib/python3.8/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.13.*) (1.3)\n",
      "Requirement already satisfied: regex in /Applications/anaconda3/lib/python3.8/site-packages (from sacrebleu->tf-models-official==2.13.*) (2021.4.4)\n",
      "Requirement already satisfied: colorama in /Applications/anaconda3/lib/python3.8/site-packages (from sacrebleu->tf-models-official==2.13.*) (0.4.4)\n",
      "Requirement already satisfied: lxml in /Applications/anaconda3/lib/python3.8/site-packages (from sacrebleu->tf-models-official==2.13.*) (4.6.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /Applications/anaconda3/lib/python3.8/site-packages (from sacrebleu->tf-models-official==2.13.*) (0.9.0)\n",
      "Requirement already satisfied: portalocker in /Applications/anaconda3/lib/python3.8/site-packages (from sacrebleu->tf-models-official==2.13.*) (2.8.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Applications/anaconda3/lib/python3.8/site-packages (from seqeval->tf-models-official==2.13.*) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.13.*) (2.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Applications/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.13.*) (1.3.2)\n",
      "Requirement already satisfied: click in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (7.1.2)\n",
      "Requirement already satisfied: importlib-resources in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (6.0.1)\n",
      "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (1.3.0)\n",
      "Requirement already satisfied: array-record in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (0.4.0)\n",
      "Requirement already satisfied: toml in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (0.10.2)\n",
      "Requirement already satisfied: tensorflow-metadata in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (1.14.0)\n",
      "Requirement already satisfied: promise in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (2.3)\n"
     ]
    }
   ],
   "source": [
    "# Use use the AdamW optimizer from https://github.com/tensorflow/models.\n",
    "!pip install \"tf-models-official==2.13.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nesisary libraries\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text \n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import keras\n",
    "import keras.utils\n",
    "import keras.layers\n",
    "import keras.models\n",
    "import keras.optimizers\n",
    "from keras import utils as np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataset directory structure\n",
    "# This will make it easy to organize and accesss our data in our directory structure\n",
    "\n",
    "\n",
    "dataset_dir = '../data/amazon_reviews'\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Make it easy to access 'train' and 'test' directories inside the dataset directory\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "\n",
    "# This will build the 'train' and 'test' directories if they haven't been built yet\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# This will build the folders 1, 2, 3, 4, 5 inside both the train and test directories\n",
    "one_dir_train = os.path.join(train_dir, '1')\n",
    "two_dir_train = os.path.join(train_dir, '2')\n",
    "three_dir_train = os.path.join(train_dir, '3')\n",
    "four_dir_train = os.path.join(train_dir, '4')\n",
    "five_dir_train = os.path.join(train_dir, '5')\n",
    "os.makedirs(one_dir_train, exist_ok=True)\n",
    "os.makedirs(two_dir_train, exist_ok=True)\n",
    "os.makedirs(three_dir_train, exist_ok=True)\n",
    "os.makedirs(four_dir_train, exist_ok=True)\n",
    "os.makedirs(five_dir_train, exist_ok=True)\n",
    "one_dir_test = os.path.join(test_dir, '1')\n",
    "two_dir_test = os.path.join(test_dir, '2')\n",
    "three_dir_test = os.path.join(test_dir, '3')\n",
    "four_dir_test = os.path.join(test_dir, '4')\n",
    "five_dir_test = os.path.join(test_dir, '5')\n",
    "os.makedirs(one_dir_test, exist_ok=True)\n",
    "os.makedirs(two_dir_test, exist_ok=True)\n",
    "os.makedirs(three_dir_test, exist_ok=True)\n",
    "os.makedirs(four_dir_test, exist_ok=True)\n",
    "os.makedirs(five_dir_test, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I ordered this thinking it would be a good way...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I purchased this even after a few other review...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I wasn't a huge fan of the taste of Crunchy Nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I have been drinking Zico coconut water for so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I dont know what all the good reviews were for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text\n",
       "0      1  I ordered this thinking it would be a good way...\n",
       "1      1  I purchased this even after a few other review...\n",
       "2      1  I wasn't a huge fan of the taste of Crunchy Nu...\n",
       "3      1  I have been drinking Zico coconut water for so...\n",
       "4      1  I dont know what all the good reviews were for..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading the dataset\n",
    "# IMPORTANT!!!!\n",
    "# In order to run this project drop in the .csv data set (called: \"Reviews\") into the \"data\" folder\n",
    "# You can find the data set here:\n",
    "# https://www.google.com/url?q=https://www.kaggle.com/datasets/arhamrumi/amazon-product-reviews&sa=D&source=docs&ust=1695764896933142&usg=AOvVaw1WeATDdUdlTItgNGGldkRF\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df_extra_params = pd.read_csv(\"../data/Reviews.csv\")\n",
    "\n",
    "# Select only the \"Score\" and \"Text\" columns\n",
    "df_unbalanced = df_extra_params[[\"Score\", \"Text\"]]\n",
    "\n",
    "# Determine minimum number of reviews for any score\n",
    "min_reviews = df_unbalanced['Score'].value_counts().min()\n",
    "\n",
    "# Make sure each review score 1-5 has the same number of reviews\n",
    "df = df_unbalanced.groupby('Score').apply(lambda x: x.sample(min_reviews)).reset_index(drop=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the data into our directory structure\n",
    "# We will seperate the data into the train and test folders\n",
    "# Inside the train and test folders we have folders 1, 2, 3, 4, 5\n",
    "# This corresponds to the \"Score\" of the review\n",
    "# Split with 50/50 ratio, randomly divided \n",
    "# Also take note of the \"sample_fraction\" variable, this is used to\n",
    "# reduce the ammount of data we will feed to BERT. This will help us\n",
    "# reduce training time when expiramenting with BERT.\n",
    "\n",
    "\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Path to the base folder\n",
    "base_folder = '../data/amazon_reviews'\n",
    "\n",
    "# Delete existing reviews from train and test folders\n",
    "for score_folder in ['1', '2', '3', '4', '5']:\n",
    "    train_folder_path = f'{base_folder}/train/{score_folder}'\n",
    "    test_folder_path = f'{base_folder}/test/{score_folder}'\n",
    "\n",
    "    shutil.rmtree(train_folder_path, ignore_errors=True)\n",
    "    shutil.rmtree(test_folder_path, ignore_errors=True)\n",
    "\n",
    "    # Recreate the train and test folders\n",
    "    os.makedirs(train_folder_path)\n",
    "    os.makedirs(test_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "# Determine the fraction for sampling (1/40th of the entire dataset)\n",
    "# Tip: for extra speedy runtimes, make this 1/40\n",
    "sample_fraction = 1/80\n",
    "\n",
    "# Separate all data into 5 dfs for scores 1-5 respectively\n",
    "df_1 = df.loc[(df[\"Score\"] == 1)]\n",
    "df_2 = df.loc[(df[\"Score\"] == 2)]\n",
    "df_3 = df.loc[(df[\"Score\"] == 3)]\n",
    "df_4 = df.loc[(df[\"Score\"] == 4)]\n",
    "df_5 = df.loc[(df[\"Score\"] == 5)]\n",
    "\n",
    "# Splitting data into test and train folders with a 50/50 ratio\n",
    "df_1_train = df_1.sample(frac=sample_fraction, replace=False, random_state=1)\n",
    "df_1_test = df_1[~df_1.isin(df_1_train)].dropna(how='all')\n",
    "\n",
    "df_2_train = df_2.sample(frac=sample_fraction, replace=False, random_state=1)\n",
    "df_2_test = df_2[~df_2.isin(df_2_train)].dropna(how='all')\n",
    "\n",
    "df_3_train = df_3.sample(frac=sample_fraction, replace=False, random_state=1)\n",
    "df_3_test = df_3[~df_3.isin(df_3_train)].dropna(how='all')\n",
    "\n",
    "df_4_train = df_4.sample(frac=sample_fraction, replace=False, random_state=1)\n",
    "df_4_test = df_4[~df_4.isin(df_4_train)].dropna(how='all')\n",
    "\n",
    "df_5_train = df_5.sample(frac=sample_fraction, replace=False, random_state=1)\n",
    "df_5_test = df_5[~df_5.isin(df_5_train)].dropna(how='all')\n",
    "\n",
    "# Converting df values into txt files, putting them in the correct folders\n",
    "def save_reviews_to_folder(df, folder_path, score):\n",
    "    for index, row in df.iterrows():\n",
    "        path = f'{folder_path}/review{index}.txt'\n",
    "        with open(path, 'a') as f:\n",
    "            txt_in = row[\"Text\"]\n",
    "            f.write(txt_in)\n",
    "\n",
    "# Define folder paths\n",
    "train_folder_base = '../data/amazon_reviews/train'\n",
    "test_folder_base = '../data/amazon_reviews/test'\n",
    "\n",
    "# Save reviews to train and test folders\n",
    "save_reviews_to_folder(df_1_train, f'{train_folder_base}/1', 1)\n",
    "save_reviews_to_folder(df_1_test, f'{test_folder_base}/1', 1)\n",
    "\n",
    "save_reviews_to_folder(df_2_train, f'{train_folder_base}/2', 2)\n",
    "save_reviews_to_folder(df_2_test, f'{test_folder_base}/2', 2)\n",
    "\n",
    "save_reviews_to_folder(df_3_train, f'{train_folder_base}/3', 3)\n",
    "save_reviews_to_folder(df_3_test, f'{test_folder_base}/3', 3)\n",
    "\n",
    "save_reviews_to_folder(df_4_train, f'{train_folder_base}/4', 4)\n",
    "save_reviews_to_folder(df_4_test, f'{test_folder_base}/4', 4)\n",
    "\n",
    "save_reviews_to_folder(df_5_train, f'{train_folder_base}/5', 5)\n",
    "save_reviews_to_folder(df_5_test, f'{test_folder_base}/5', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n",
      "372\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick sanity check to make sure our reviews are \n",
    "# evenly distributed in our directory\n",
    "\n",
    "import os\n",
    "\n",
    "folder_path = '../data/amazon_reviews/train/4'\n",
    "\n",
    "# Get the list of items in the folder\n",
    "items = os.listdir(folder_path)\n",
    "\n",
    "# Print the number of items in the folder\n",
    "print(len(items))\n",
    "\n",
    "\n",
    "# repeat but with a different reviews folder\n",
    "folder_path = '../data/amazon_reviews/train/3'\n",
    "\n",
    "items = os.listdir(folder_path)\n",
    "\n",
    "print(len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1860 files belonging to 5 classes.\n",
      "Using 1488 files for training.\n",
      "Found 1860 files belonging to 5 classes.\n",
      "Using 372 files for validation.\n",
      "Found 146985 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Now we will use the text_dataset_from_directory utility to create a labeled tf.data.Dataset.\n",
    "# Let's create a validation set using an 80:20 split of the training data by\n",
    "# using the validation_split argument below.\n",
    "\n",
    "# Note: When using the validation_split and subset arguments\n",
    "# make sure to either specify a random seed, or to pass shuffle=False, \n",
    "# so that the validation and training splits have no overlap.\n",
    "\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    '../data/amazon_reviews/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    labels='inferred',\n",
    "    subset='training',\n",
    "    seed=seed,\n",
    "    label_mode='int')\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    '../data/amazon_reviews/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    labels='inferred',\n",
    "    subset='validation',\n",
    "    seed=seed,\n",
    "    label_mode='int')\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    '../data/amazon_reviews/test',\n",
    "    batch_size=batch_size,\n",
    "    labels='inferred',\n",
    "    label_mode='int')\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(None,), dtype=tf.int32, name=None)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentative function for one hot encoding NOT RELIABLE \n",
    "def _map_func(text, labels):\n",
    "    # i=0\n",
    "    # labels_enc = tf.TensorArray(tf.float32, size=0, dynamic_size=True,clear_after_read=False)\n",
    "    # for text_batch, label_batch in train_ds.take(1):\n",
    "    #     label = label_batch.numpy()[i]\n",
    "    #     if label==0:\n",
    "    #         label = 1\n",
    "    #     elif label==1:\n",
    "    #         label = 2\n",
    "    #     elif label==2:\n",
    "    #         label = 3\n",
    "    #     elif label==3:\n",
    "    #         label = 4\n",
    "    #     else: \n",
    "    #         label = 5\n",
    "    #     print(f'Label : {label} ({class_names[label]})')\n",
    "    # for label in labels:\n",
    "    #     if label.numpy()[i]==0:\n",
    "    #         label.numpy()[i] = 1\n",
    "    #     elif label.numpy()[i]==1:\n",
    "    #         label.numpy()[i] = 2\n",
    "    #     elif label.numpy()[i]==2:\n",
    "    #         label.numpy()[i] = 3\n",
    "    #     elif label.numpy()[i]==3:\n",
    "    #         label.numpy()[i] = 4\n",
    "    #     else: \n",
    "    #         label.numpy()[i] = 5\n",
    "        # label = tf.one_hot(label, 5, name='label', axis=-1)\n",
    "        # labels_enc.write(i, label)\n",
    "        # i+=1\n",
    "\n",
    "#     return text, labels\n",
    "    pass\n",
    "# train_ds = train_ds.map(_map_func)\n",
    "# test_ds = test_ds.map(_map_func)\n",
    "# val_ds = val_ds.map(_map_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b'I love green tea, I love Kit Kats, but the two do not belong together. I hate the after taste of them.'\n",
      "Label : 0 (1)\n",
      "Review: b'No DOP or Consorzio San Marzano stamp but this product is outstanding compaired to the cost.<br />Arrived well packaged without damage,'\n",
      "Label : 4 (5)\n",
      "Review: b\"I grew up eating these yummy chocolate malt balls in Hong Kong.  The chocolate is rich and creamy while the honeycomb malt center is so crisp!  I love them!  I tried the Whoppers and they were no comparison to the Maltesers.  Too bad they're so hard to find and sold out here on Amazon.  My local Cost Plus World Market carries them.\"\n",
      "Label : 4 (5)\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(3):\n",
    "    print(f'Review: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32,), dtype=string, numpy=\n",
      "array([b\"I ordered this item back in June. After waiting about to weeks & with no type of update when viewing the tracking information, I figured the Ginger Ale just wasn't coming. So I'm giving it 1 star for looks. It's a nice bottle.\",\n",
      "       b'If you are thinking about buying these, you could probably just get away with going to your local grocery store and picking up a bag of Chips! Ahoy cookies instead. Yes, these are a bit more substantial, but the taste is almost exactly the same. There is definitely nothing \"luxury\" about these cookies. I bought these because I loved the Cadbury Oatmeal and Chocolate Chip cookies. Sadly, these did nothing but disappoint.',\n",
      "       b\"My dogs happily ate this food, had no digestive issues from the change and for the price, it's not bad.  Still, if you can afford to spend more, you can find a more nutritious food.\",\n",
      "       b'Use with caution. When my baby was 5 weeks old I noticed that my milk supply was decreasing. I was taking 2 pills 3 times a day for about a week when I started felling really sick. I head strong headaches, I was weak and I had problem with breathing. My baby had a cough and was spitting up milk. She could not swallow, the mucus in her mouth was very thick. I learned from my doctor that fenugreek has very dangerous side effects: it can cause asthma;  reduces blood glucose levels which means it is not right for diabetics; promotes menstrual flow and can make yor menstrual cycles abnormal. I spent money on pills, teas, drops and my milk supply did not increase at all. I can breastfeed my baby only once a day now. I am happy with what I can give her although I see clearly that she hates formula. I give only one star because the manufacturer does not warn about side effects the product can have on mother and her child. I bet they know the pills are used mostly for the purpose of increasing breastmilk supply, yet they do not care about us.',\n",
      "       b'As others have noted, these treats are made in China as clearly stated on the bag. I didn\\'t like that, but the news gets worse.<br /><br />On the back of the bag is a statement: \"Not for human consumption. Wash hands with soap and water after handling.\"<br /><br />If I have to wash my hands after handling these things, I\\'m sure as hell not going to feed them to my Yorkies!<br /><br />As expected, Amazon graciously accepted a return.',\n",
      "       b'This tea is ideal for anyone who likes a cool glass of iced tea but does not have access to ice. It has become my go-to tea after years of enjoying the myriad loose-leaf teas from Teavana and other such places. Why? Let me share my story...<br /><br />My daily routine was simple: brew a liter of tea in the morning to bring to work, pour it over ice throughout the day and enjoy several large glasses of fresh and tasty iced tea. Then the refrigerator went out, and we no longer have ice at work. So I spent months choosing my lunch based on what restaurants would give me a large iced tea to take back to the office.<br /><br />Well, no more of that! Now I keep bags of <a href=\"http://www.amazon.com/gp/product/B0028GWGYW\">Twinings English Classic Cold Brewed Tea, 20-Count Packages (Pack of 6)</a> on hand, whether at my desk or while traveling, and I just fill a glass from the water cooler and minutes later I am enjoying a wonderful iced tea with no bitterness at all.<br /><br />I have used one tea bag throughout the day to make 5 or 6 large glasses of tea. Don\\'t be afraid to let it steep as long as you like.',\n",
      "       b\"Tried the Mango Macadamia this time and will not get them again.  Really like the Macadamia Apricot the best of all I've tried thus far.\",\n",
      "       b\"Haribo makes a wide range of candies, including some world class goodies, but this is not one of them. The frogs are made of a hard gummi, and are all green except for a thin white layer on the frogs' bellies. You might not recognize them as frogs unless you're told. There is no discernible taste or texture difference between the white and green substances, and in any case, the flavor is unidentified, unidentifiable, and not very good. It definitely is not lime, as advertised. Maybe somebody likes this stuff, but I didn't. I set the bag out at the office after my first sample.\",\n",
      "       b\"Like most commercial dog foods, Eukanuba is mostly garbage, full of by- products, fillers and grains that dogs do not need for health. While this is certainly not the worst dog food out there, it is far from the best and absolutely not worth anywher near the price it commands. Pet guardians, please take the time to educate yourselves on what goes into your pet's food. That initiative will give them the gift of a longer, healthier life, eliminate many chronic health issues that pets face, save you money in vet bills in the long run... the list of benefits goes on and on.<br /><br />I have done extensive research on canine and feline nutrition and I highly recommend, if you are going to feed commercial food, Welless, California Naturals or Innova. For a great complete meal, I give my dogs all one of those dry foods mixed with a little bit of canned from the same brands, water for hydration, a touch of a good quality essential oil (such as olive, flax, etc.), a touch of powdered kelp and alfalfa, a splash of organic apple cider vinegar, Prozyme digestive enzymes to unlock the nutrients in all that good food and top with a dollop of plain yogurt. It sounds like a lot, but it takes me less than 5 minutes to assemble the meals for 6 dogs - and they are ridiculously healthy an full of energy, even our 3 seniors who are 11, 12 and 13.\",\n",
      "       b\"Awful, Awful, Awful!  I like a lighter roast and I really love french vanilla so I was thrilled to find this one.  With the first sip I was highly disappointed.  It has a very strong artificial flavor that doesn't taste anything like french vanilla and it certainly doesn't taste like coffee.  Like one other reviewer said, I'll probably just toss it in the trash and throw that money away because I don't know anybody who would enjoy it.  YUK!\",\n",
      "       b\"I sent these to a friend of mine who fell in love with them when we studied abroad. They arrived on time and apparently quite edible because she has been loving them! Only downside is the seller doesn't allow for any sort of note enclosure to let the recipient know who its from. Luckily this gift was a no-brainer as she knew I sent it.  Add in the option of allowing a written message and I'd give this 5 stars.\",\n",
      "       b\"It just doesn't taste like butter to me. Highly concentrated or no, it tasted closer to garlic or something. Maybe it's just me, but this stuff was awful.\",\n",
      "       b\"My daughter will eat just about any flavor from earths best, incl. Fruits, vegetables and soups. But for some reason she won't eat this one. The consistency is thicker and chunkier than the others. At first I thought it was because she wasn't ready for stage 3. But now that she is 10 months old and is used to eating thicker soups and other foods, she still refuses this one. It tastes fine to me, go figure\",\n",
      "       b\"Beautiful hair starts with healthy hair. Although I've been very happy with Pantene for some time, I was very interested when I saw CLEAR SCALP & HAIR BEAUTY Volumizing Root Boost Nourishing Conditioner. I expected something light and effective without much of an aroma.<br /><br />The conditioner works well enough, but is not any better than my Pantene products. Worse, it has a heavy perfume smell that I don't like at all.<br /><br />This conditioner definitely leaves the hair soft and tangle free, but my hair has no more volume than when I use Pantene.<br /><br />I will finish this bottle, but will go back to Pantene when I am done. I like its clean, fresh scent much better than this one.\",\n",
      "       b'I recently cut fiber one bars out of my diet after I noticed that I get all the fiber I need in a day just by eating whole wheat bread, fruits, vegetables and nuts. If you don\\'t have many of the aforementioned foods in your daily diet, then one or two of these a day may be a good fiber supplement. However, this comes at the cost of making you extremely gassy all through the day, which can be embarrassing and sometimes even annoying when it\\'s more frequent than the occasional toot that slips out. IMHO, it\\'s better to get the daily dietary fiber your body needs via the more natural means that I mentioned above.<br /><br />Another thing to keep in mind, especially when dieting, is that these bars are pretty high in sugar. (As much as 10g added sugar per bar!) Considering a \"healthy\" amount of added sugar per day maxes out at about 40-50g per day, eating a few of these bars will chew right through that cap in no time. There\\'s also emerging research showing that eating high-sugar foods (like fiber one bars) will make you crave more sweets, leading you to eat or drink even more unhealthy things like candy bars and sodas. So if you\\'re on a diet, eating these bars probably aren\\'t as great for your diet as you think!<br /><br />I\\'ve been fiber one free for two weeks now and I\\'m happy for the decision. (And so are my friends!)',\n",
      "       b\"I love silken tofu and use it a lot to make smoothies for myself and my toddlers. This is good-quality silken tofu and I was happy with the price and convenience of having it delivered to my door. (It's hard to find in grocery stores around here!)\",\n",
      "       b\"A tea expert I am not. However, my husband and I recently switched from coffee in the morning to English breakfast yea and we drink green tea with dinner and since we use and like Higgins & Burke green tea, we were anxious to try their Earl Grey. We've had Earl Grey in the past and I'm not sure why we settled on breakfast tea instead with breakfast, probably just because of the name, you know breakfast tea with breakfast. But now, after having tried this, we're switching. We drink it with milk and it like it very much.\",\n",
      "       b\"I've bought these pineapple Crunchies in my local grocery store and they were good (though very expensive at about 6 bucks per serving). So when I ordered the six-pack of them here, I expected them to taste just as good. That was not the case at all. The outside of each package was sticky but that didnt bother me much. What bothered me is that the actual product itself was stale. Each piece (in 4 of the 6 bags I opened) was rubbery. Chewing the Crunchies only compressed it into a solid wad of what then seemed like hard plastic. The result was something that simply could not be broken up and swallowed. I also noticed that the built-in ziplock seal didnt work on these bags. After 4 bags, I tossed out the rest and contacted the manufacturer via email (twice). I never received a reply. I think the Crunchies people are selling Amazon bad batches. Avoid these at all cost!\",\n",
      "       b\"Although this product is better for you than pasta, nothing beats the taste of real pasta. I just can't get past the texture of these noodles, they are chewy and gummy and gelatin all at the same time.\",\n",
      "       b'My dog is not that interested in them. She ignores them sometimes, and at other times eats them. All in all she does not seem too interested in them. She is not a finicky dog. When fed her morning and afternoon meal she wolfs them down, same with all her other treats. I was hoping she would do the same with these but she mouths it and then drops it. She usually has Innova , but I have not seen her turn away any dog food offered. Because she is not choosy I was very surprised with the high ratings to find her leaving these on the ground and walking away. If I keep giving it to her she will eventually eat them. All these glowing reviews bring to mind a cliche, sometimes some thing to good to be true is just that!',\n",
      "       b'Delivery was fast, and my order arrived intact and unbroken. The product was of very good quality.',\n",
      "       b\"I try to eat healthy and shop around for tempting treats; well, in this case I stumbled upon Stacy's Pita Chips, Cinnamon Sugar and because of the reviews and my partiality to anything Cinnamon and Sugar I bit. The outcome was less than spectacular. I short, I ended up paying paying $22.00 + for a case of chips (24 bags in all) in which I really don't care for. Man, I always kick myself when this happens! Health food is so expensive and I'm stuck with something you'd only eat if you were at a vending machine and it was the only thing left there to buy. Honestly, it's not that Stacy's  Cinnamon & Sugar Pita Chips are nasty, it's just that they are to bland for my palate. Imagine dry Cinnamon Toast Crunch cereal only on a baked Pita Chip and with much less of a Cinnamon and Sugar taste. Some people may like the more bland taste. I do not.\",\n",
      "       b'First of all, I think it is amazing that there is such a thing as \"defatted cocoa\", and the product is a good idea despite the fact that it contains \"trace calories\" (check the back of the bottle).  However, this does not at all taste like chocolate in my opinion.  I couldn\\'t really use it for dipping fruit; I couldn\\'t use it for chocolate milk or chocolate sodas.  I have a feeling that this may work if you add it to creamy things like non-diet ice cream.  It left a weird after-taste in my mouth, and in general had a chemical taste. I will not buy this product again.  I\\'d rather eat normal chocolate sparingly than this stuff in larger quantities.',\n",
      "       b'The Icicle is an excellent product, works as advertised. I think it\\'s the easiest way to attach a quality mic directly to your computer.<br />the icicle is a fine \"Plug & Play\" mic interface.',\n",
      "       b'Or Cappucino I guess.<br /><br />I\\'m not a big coffee \"aficianado\" or basically someone who is snooty about coffee.  I can appreciate a good cup of joe.  I do know many gourmet coffees in the US are just average and many store varieties are basically sewage water.  I\\'ve experienced coffee that is served in a coffee shop where the beans are picked literally in the \"back yard\" (humid subtropical country).<br /><br />This isn\\'t on that level of coffee, but it\\'s surprisingly more nearer to it than anything I\\'ve bought so far.  Starbucks included.  Taste wise (coffee flavor) it tastes great.<br /><br />Made with cellulose gel and gum and carrageenan to give it some body, else it would just be flavored what, as it would be normally.  But you gotta make a product more appealing for the masses ;)',\n",
      "       b\"I don't know what it tastes like. I just know that that this item came up on a gluten free search but it is not gluten free. It contains semolina and semolina is wheat.\",\n",
      "       b'Good price but the coffee lacks aroma and flavor. To be a Colombian you expect more Arabic qualities. Also require extra ammount to get a decent flavor.',\n",
      "       b'Mostly for the novelty, I have tried many, many of the packaged, pre-ground foreign coffees available on Amazon, and I must say that very few of them are worth your time or money.  They have obviously been sitting around or in transit for a very long time, and you would be far better off just buying the same crummy coffee at the super market where it would probably be much fresher.  The packaging is certainly cool and fun, but it just is not worth the awful taste and very high shipping prices.',\n",
      "       b'I love the original lemonade & Black Tea Rehab so I gave this a try. It has the same light and non-syrupy taste and texture which is great. The taste is lacking, just a bit too dull. It is a mildly sweet taste. I could easily detect the pineapple juice which was good. Worth picking up a gas station to try but it would not be the top drink I would recommend.',\n",
      "       b\"I tried this coffee once and I am throwing away the rest. The taste is very artificial.<br />Schuil's coffee also has cherry flavor and it is so much better.\",\n",
      "       b'This is one of the most tasteless things I have ever tasted. If I could have sent them back, I would have. CAS',\n",
      "       b\"For those of us used to Tuna in the can this product seems very unique and certainly economical if you have several people in your family.<br />What you are getting is a single solid slab of tuna measuring approx. 6 inches wide 9 inches long and 1 inch thick. 2 whole pounds of Wild Planet's low mercury wild caught tuna. As you can see from the reviews of their canned Tuna this really is a very delicious Tuna and they cook it right in the aluminum type pouch that it comes in. When you open the pouch the juices from that cooking are evident. I know many of you probably have bought fresh Tuna in the fish market and find canned Tuna of no comparison. I, myself have only done that once and it turned out very dry and leathery when broiled. I don't know how to compare a Tuna loin like this to a fresh cooked piece of Tuna but if you don't have access to fresh tuna or just want a high quality Tuna for sandwiches or other dishes I truly don't know where you could find a higher quality and certainly more economical piece of fish. This truly is a novel idea whose time has come. Taking a large, fresh piece of fish and putting it in a sealed pouch and cooking it. It is the next best thing to fresh fish, especially when your favorite fish is not in season or available frozen.<br />I wish someone would package Sockeye Salmon like this. The only ingredients in this product are Tuna and sea salt. None of the extenders etc.. in commercial store bought Tuna. Now, Wild Planet's new can's of Tuna are 5 ounces, so you would have to buy 6 1/3 cans to equal this 2 lb. pouch. A cost of $27.00 verses $16.00. So you can see it certainly is a better bargin. The only caveat to ordering this product in such a large size is that once you open it you only have x amount of days to eat it. No problem for a family of 3 or 4 but if it is just one person it may prove to be a bit to much when all you want is one Tuna sandwich, but, I have found that by eating a little each day I can finish a package in 4 days just by having a piece once a day. It's a great way to get some extra protein and omega 3's and I really look forward to having it because it tastes so good. I hope this was helpful.\"],\n",
      "      dtype=object)>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([0, 1, 3, 0, 0, 4, 2, 1, 1, 0, 3, 0, 1, 2, 1, 4, 4, 0, 1, 1, 3, 2,\n",
      "       2, 4, 3, 0, 2, 1, 3, 0, 0, 4], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "for row in train_ds.take(1):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to 1\n",
      "Label 1 corresponds to 2\n"
     ]
    }
   ],
   "source": [
    "# Looking at what labels correspond to what ratings\n",
    "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "# Now we will load up a model with TensorFlow Hub\n",
    "# We will be using a small BERT to start with\n",
    "# to read about all the BERT model available click this link\n",
    "# https://colab.research.google.com/drive/1UytfDnUpCQTHK8BA8n__B4YCWm4gzIeW#scrollTo=dX8FtlpGJRE6\n",
    "\n",
    "\n",
    "\n",
    "#@title Choose a BERT model to fine-tune\n",
    "\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models.\n",
    "\n",
    "The preprocessing model must be the one referenced by the documentation of the BERT model.\n",
    "\n",
    "Note: We will load the preprocessing model into a hub.KerasLayer to compose your fine-tuned model. This is the preferred API to load a TF2-style SavedModel from TF Hub into a Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the preprocessing model on some text and see the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_mask', 'input_word_ids', 'input_type_ids']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [ 101 2023 2003 2107 2019 6429 4031  999  102    0    0    0]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "text_test = ['this is such an amazing product!']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, now you have the 3 outputs from the preprocessing that a BERT model would use (input_words_id, input_mask and input_type_ids).\n",
    "\n",
    "Some other important points:\n",
    "\n",
    "The input is truncated to 128 tokens. The number of tokens can be customized, and you can see more details on the Solve GLUE tasks using BERT on a TPU colab.\n",
    "The input_type_ids only have one value (0) because this is a single sentence input. For a multiple sentence input, it would have one number for each input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the BERT model\n",
    "\n",
    "Before putting BERT into our model, let's take a look at its outputs. We will load it from TF Hub and see the returned values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Pooled Outputs Shape:(1, 512)\n",
      "Pooled Outputs Values:[ 0.2560776   0.997757   -0.44676325  0.2663918   0.2193461   0.43699053\n",
      "  0.9671976  -0.9815668  -0.12048521 -0.98637974  0.09855995 -0.9820184 ]\n",
      "Sequence Outputs Shape:(1, 128, 512)\n",
      "Sequence Outputs Values:[[-0.37835765  0.79211026  0.17865962 ... -0.02951723  0.53888327\n",
      "  -0.10518374]\n",
      " [-0.22593209  0.5280527  -0.01349533 ...  0.7996965  -0.2248629\n",
      "   0.9144497 ]\n",
      " [-0.7240933   1.1752062  -0.8487482  ...  0.13510218 -0.39534006\n",
      "   0.7117282 ]\n",
      " ...\n",
      " [-0.05767796  0.0561219  -0.17037475 ...  0.46868354  0.7525876\n",
      "   0.5103831 ]\n",
      " [-0.10215646  0.1487371  -0.13012674 ...  0.43205702  1.0691458\n",
      "   0.272215  ]\n",
      " [-0.394735    0.60435593 -0.13972646 ... -0.07493115  1.1170473\n",
      "   0.25230798]]\n"
     ]
    }
   ],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT models return a map with 3 important keys: pooled_output, sequence_output, encoder_outputs:\n",
    "\n",
    "pooled_output represents each input sequence as a whole. The shape is [batch_size, H]. You can think of this as an embedding for the entire movie review.\n",
    "sequence_output represents each input token in the context. The shape is [batch_size, seq_length, H]. You can think of this as a contextual embedding for every token in the movie review.\n",
    "encoder_outputs are the intermediate activations of the L Transformer blocks. outputs[\"encoder_outputs\"][i] is a Tensor of shape [batch_size, seq_length, 1024] with the outputs of the i-th Transformer block, for 0 <= i < L. The last value of the list is equal to sequence_output.\n",
    "For the fine-tuning you are going to use the pooled_output array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your model\n",
    "You will create a very simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net =  tf.keras.layers.Dense(15, activation = 'relu')(net)\n",
    "  net = tf.keras.layers.Dense(5, activation='softmax', name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the model runs with the output of the preprocessing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.5516175  0.56160784 0.54550207 0.5662174  0.52402   ]], shape=(1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is meaningless, of course, because the model has not been trained yet.\n",
    "\n",
    "Let's take a look at the model's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(classifier_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "You now have all the pieces to train a model, including the preprocessing module, BERT encoder, data, and classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "metrics = tf.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "For fine-tuning, let's use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as AdamW.\n",
    "\n",
    "For the learning rate (init_lr), you will use the same schedule as BERT pre-training: linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (num_warmup_steps). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the BERT model and training\n",
    "Using the classifier_model you created earlier, you can compile the model with the loss, metric and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: training time will vary depending on the complexity of the BERT model you have selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 507s 10s/step - loss: 1.6363 - sparse_categorical_accuracy: 0.2426 - val_loss: 1.5418 - val_sparse_categorical_accuracy: 0.3172\n",
      "Epoch 2/3\n",
      "47/47 [==============================] - 447s 9s/step - loss: 1.5043 - sparse_categorical_accuracy: 0.3414 - val_loss: 1.4745 - val_sparse_categorical_accuracy: 0.3468\n",
      "Epoch 3/3\n",
      "47/47 [==============================] - 474s 10s/step - loss: 1.4139 - sparse_categorical_accuracy: 0.3972 - val_loss: 1.4580 - val_sparse_categorical_accuracy: 0.3629\n"
     ]
    }
   ],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=train_ds,\n",
    "                               validation_data=val_ds,\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  41/4594 [..............................] - ETA: 3:50:43 - loss: 0.0000e+00 - categorical_accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-9610144f1591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Loss: {loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Accuracy: {accuracy}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2198\u001b[0m                         ):\n\u001b[1;32m   2199\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2200\u001b[0;31m                             logs = test_function_runner.run_step(\n\u001b[0m\u001b[1;32m   2201\u001b[0m                                 \u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m                                 \u001b[0mdata_handler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   3998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3999\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4000\u001b[0;31m         \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4001\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4002\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    862\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m       (concrete_function,\n\u001b[1;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1348\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'amazon_reviews'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from the saved model:\n",
      "input: This is the best product I have ever bought in my life! SO amazing!! : score: 0.515502\n",
      "input: The product was great!         : score: 0.538779\n",
      "input: The product was meh.           : score: 0.605659\n",
      "input: This was the absolute worst thing I have ever bought. I hate this product. : score: 0.616166\n",
      "input: The product was so amazing! I love it!!! : score: 0.516273\n",
      "\n",
      "Results from the model in memory:\n",
      "input: This is the best product I have ever bought in my life! SO amazing!! : score: 0.515502\n",
      "input: The product was great!         : score: 0.538779\n",
      "input: The product was meh.           : score: 0.605659\n",
      "input: This was the absolute worst thing I have ever bought. I hate this product. : score: 0.616166\n",
      "input: The product was so amazing! I love it!!! : score: 0.516273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_my_examples(inputs, results):\n",
    "  result_for_printing = \\\n",
    "    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'for i in range(len(inputs))]\n",
    "  print(*result_for_printing, sep='\\n')\n",
    "  print()\n",
    "\n",
    "\n",
    "examples = [\n",
    "    'This is the best product I have ever bought in my life! SO amazing!!',  # this is the same sentence tried earlier\n",
    "    'The product was great!',\n",
    "    'The product was meh.',\n",
    "    'This was the absolute worst thing I have ever bought. I hate this product.',\n",
    "    'The product was so amazing! I love it!!!'\n",
    "]\n",
    "\n",
    "reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
    "original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n",
    "\n",
    "print('Results from the saved model:')\n",
    "print_my_examples(examples, reloaded_results)\n",
    "print('Results from the model in memory:')\n",
    "print_my_examples(examples, original_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
