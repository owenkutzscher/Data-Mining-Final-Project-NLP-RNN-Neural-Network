{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "# After doing some research, we will want to use TensorFlow, prob Keras (a deep\n",
    "# learning API written on top of TensorFlow, it's currently being used\n",
    "# in the LHC (Large Hadron Collider)).\n",
    "\n",
    "# We will be classifying text with BERT:\n",
    "# https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
    "\n",
    "# NOTE: as of 10/19/23 tensorflow will not run on windowns\n",
    "# I recomend running this through jupyterlab on a linux kernal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-text==2.13.* in /Applications/anaconda3/lib/python3.8/site-packages (2.13.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-text==2.13.*) (0.15.0)\n",
      "Requirement already satisfied: tensorflow<2.14,>=2.13.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-text==2.13.*) (2.13.1)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.13.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.15.0)\n",
      "Requirement already satisfied: packaging in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (23.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.10.0)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (23.5.26)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (52.0.0.post20210125)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (16.0.6)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.13.1)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.23.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.59.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.4.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.20.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.34.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Applications/anaconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.36.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.7.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.25.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (5.3.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Applications/anaconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Applications/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Applications/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.5.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.26.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "# A dependency of the preprocessing for BERT inputs\n",
    "!pip install -U \"tensorflow-text==2.13.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-models-official==2.13.* in /Applications/anaconda3/lib/python3.8/site-packages (2.13.2)\n",
      "Requirement already satisfied: kaggle>=1.3.9 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.5.16)\n",
      "Requirement already satisfied: sacrebleu in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.3.1)\n",
      "Requirement already satisfied: tf-slim>=1.1.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.1.0)\n",
      "Requirement already satisfied: Pillow in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (8.2.0)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.2.4)\n",
      "Requirement already satisfied: seqeval in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.2.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.10.1)\n",
      "Requirement already satisfied: matplotlib in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (3.3.4)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.104.0)\n",
      "Requirement already satisfied: Cython in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (0.29.23)\n",
      "Requirement already satisfied: tensorflow~=2.13.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.13.1)\n",
      "Requirement already satisfied: opencv-python-headless in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (4.8.1.78)\n",
      "Requirement already satisfied: sentencepiece in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (0.1.99)\n",
      "Requirement already satisfied: six in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (0.7.5)\n",
      "Requirement already satisfied: oauth2client in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (4.1.3)\n",
      "Requirement already satisfied: numpy>=1.20 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.23.0)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (5.8.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (6.0.1)\n",
      "Requirement already satisfied: gin-config in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (0.5.0)\n",
      "Requirement already satisfied: pycocotools in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.0.7)\n",
      "Requirement already satisfied: py-cpuinfo>=3.3.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (9.0.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (0.15.0)\n",
      "Requirement already satisfied: immutabledict in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (3.0.0)\n",
      "Requirement already satisfied: tensorflow-text~=2.13.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-datasets in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (4.9.2)\n",
      "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.23.3)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (0.22.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.12.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (0.1.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (4.1.1)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.25.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (3.20.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (1.61.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Applications/anaconda3/lib/python3.8/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.4.7)\n",
      "Requirement already satisfied: python-slugify in /Applications/anaconda3/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (8.0.1)\n",
      "Requirement already satisfied: bleach in /Applications/anaconda3/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (3.3.0)\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (4.59.0)\n",
      "Requirement already satisfied: python-dateutil in /Applications/anaconda3/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (2.8.1)\n",
      "Requirement already satisfied: urllib3 in /Applications/anaconda3/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (1.26.4)\n",
      "Requirement already satisfied: certifi in /Applications/anaconda3/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (2020.12.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Applications/anaconda3/lib/python3.8/site-packages (from pandas>=0.22.0->tf-models-official==2.13.*) (2021.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Applications/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (0.5.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (4.0.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (3.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.59.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (16.0.6)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (23.5.26)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.3.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.34.0)\n",
      "Requirement already satisfied: packaging in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (23.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.10.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.4.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (4.5.0)\n",
      "Requirement already satisfied: setuptools in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (52.0.0.post20210125)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Applications/anaconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow~=2.13.0->tf-models-official==2.13.*) (0.36.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (0.7.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (1.0.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Applications/anaconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Applications/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.4.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.2.2)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.13.*) (0.1.8)\n",
      "Requirement already satisfied: webencodings in /Applications/anaconda3/lib/python3.8/site-packages (from bleach->kaggle>=1.3.9->tf-models-official==2.13.*) (0.5.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->tf-models-official==2.13.*) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->tf-models-official==2.13.*) (1.3.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /Applications/anaconda3/lib/python3.8/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.13.*) (1.3)\n",
      "Requirement already satisfied: regex in /Applications/anaconda3/lib/python3.8/site-packages (from sacrebleu->tf-models-official==2.13.*) (2021.4.4)\n",
      "Requirement already satisfied: portalocker in /Applications/anaconda3/lib/python3.8/site-packages (from sacrebleu->tf-models-official==2.13.*) (2.8.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /Applications/anaconda3/lib/python3.8/site-packages (from sacrebleu->tf-models-official==2.13.*) (0.9.0)\n",
      "Requirement already satisfied: colorama in /Applications/anaconda3/lib/python3.8/site-packages (from sacrebleu->tf-models-official==2.13.*) (0.4.4)\n",
      "Requirement already satisfied: lxml in /Applications/anaconda3/lib/python3.8/site-packages (from sacrebleu->tf-models-official==2.13.*) (4.6.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Applications/anaconda3/lib/python3.8/site-packages (from seqeval->tf-models-official==2.13.*) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.13.*) (2.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Applications/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.13.*) (1.3.2)\n",
      "Requirement already satisfied: array-record in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (0.4.0)\n",
      "Requirement already satisfied: promise in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (2.3)\n",
      "Requirement already satisfied: click in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (7.1.2)\n",
      "Requirement already satisfied: toml in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (0.10.2)\n",
      "Requirement already satisfied: importlib-resources in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (6.0.1)\n",
      "Requirement already satisfied: tensorflow-metadata in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (1.14.0)\n",
      "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Use use the AdamW optimizer from https://github.com/tensorflow/models.\n",
    "!pip install \"tf-models-official==2.13.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text \n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import keras\n",
    "import keras.utils\n",
    "import keras.layers\n",
    "import keras.models\n",
    "import keras.optimizers\n",
    "from keras import utils as np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataset directory structure\n",
    "# This will make it easy to organize and accesss our data in our directory structure\n",
    "\n",
    "\n",
    "dataset_dir = '../data/amazon_reviews'\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Make it easy to access 'train' and 'test' directories inside the dataset directory\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "\n",
    "# This will build the 'train' and 'test' directories if they haven't been built yet\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# This will build the folders 1, 2, 3, 4, 5 inside both the train and test directories\n",
    "one_dir_train = os.path.join(train_dir, '1')\n",
    "two_dir_train = os.path.join(train_dir, '2')\n",
    "three_dir_train = os.path.join(train_dir, '3')\n",
    "four_dir_train = os.path.join(train_dir, '4')\n",
    "five_dir_train = os.path.join(train_dir, '5')\n",
    "os.makedirs(one_dir_train, exist_ok=True)\n",
    "os.makedirs(two_dir_train, exist_ok=True)\n",
    "os.makedirs(three_dir_train, exist_ok=True)\n",
    "os.makedirs(four_dir_train, exist_ok=True)\n",
    "os.makedirs(five_dir_train, exist_ok=True)\n",
    "one_dir_test = os.path.join(test_dir, '1')\n",
    "two_dir_test = os.path.join(test_dir, '2')\n",
    "three_dir_test = os.path.join(test_dir, '3')\n",
    "four_dir_test = os.path.join(test_dir, '4')\n",
    "five_dir_test = os.path.join(test_dir, '5')\n",
    "os.makedirs(one_dir_test, exist_ok=True)\n",
    "os.makedirs(two_dir_test, exist_ok=True)\n",
    "os.makedirs(three_dir_test, exist_ok=True)\n",
    "os.makedirs(four_dir_test, exist_ok=True)\n",
    "os.makedirs(five_dir_test, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Disgusting.  This is not anything like pasta s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I bought one of the Stash teas-don't remember ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I made the  mistake of opening a package of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Tastes like a poor Ranch fav almond. Barely an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Received a pack as gift and that was the best ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text\n",
       "0      1  Disgusting.  This is not anything like pasta s...\n",
       "1      1  I bought one of the Stash teas-don't remember ...\n",
       "2      1  I made the  mistake of opening a package of th...\n",
       "3      1  Tastes like a poor Ranch fav almond. Barely an...\n",
       "4      1  Received a pack as gift and that was the best ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading the dataset\n",
    "# IMPORTANT!!!!\n",
    "# In order to run this project drop in the .csv data set (called: \"Reviews\") into the \"data\" folder\n",
    "# You can find the data set here:\n",
    "# https://www.google.com/url?q=https://www.kaggle.com/datasets/arhamrumi/amazon-product-reviews&sa=D&source=docs&ust=1695764896933142&usg=AOvVaw1WeATDdUdlTItgNGGldkRF\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df_extra_params = pd.read_csv(\"../data/Reviews.csv\")\n",
    "\n",
    "# Select only the \"Score\" and \"Text\" columns\n",
    "df_unbalanced = df_extra_params[[\"Score\", \"Text\"]]\n",
    "\n",
    "# Determine minimum number of reviews for any score\n",
    "min_reviews = df_unbalanced['Score'].value_counts().min()\n",
    "\n",
    "# Make sure each review score 1-5 has the same number of reviews\n",
    "df = df_unbalanced.groupby('Score').apply(lambda x: x.sample(min_reviews)).reset_index(drop=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the data into our directory structure\n",
    "# We will seperate the data into the train and test folders\n",
    "# Inside the train and test folders we have folders 1, 2, 3, 4, 5\n",
    "# This corresponds to the \"Score\" of the review\n",
    "# Split with 50/50 ratio, randomly divided \n",
    "# Also take note of the \"sample_fraction\" variable, this is used to\n",
    "# reduce the ammount of data we will feed to BERT. This will help us\n",
    "# reduce training time when expiramenting with BERT.\n",
    "\n",
    "\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Path to the base folder\n",
    "base_folder = '../data/amazon_reviews'\n",
    "\n",
    "# Delete existing reviews from train and test folders\n",
    "for score_folder in ['1', '2', '3', '4', '5']:\n",
    "    train_folder_path = f'{base_folder}/train/{score_folder}'\n",
    "    test_folder_path = f'{base_folder}/test/{score_folder}'\n",
    "\n",
    "    shutil.rmtree(train_folder_path, ignore_errors=True)\n",
    "    shutil.rmtree(test_folder_path, ignore_errors=True)\n",
    "\n",
    "    # Recreate the train and test folders\n",
    "    os.makedirs(train_folder_path)\n",
    "    os.makedirs(test_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "# Determine the fraction for sampling (1/40th of the entire dataset)\n",
    "# Tip: for extra speedy runtimes, make this 1/40\n",
    "sample_fraction = 1/80\n",
    "\n",
    "# Separate all data into 5 dfs for scores 1-5 respectively\n",
    "df_1 = df.loc[(df[\"Score\"] == 1)]\n",
    "df_2 = df.loc[(df[\"Score\"] == 2)]\n",
    "df_3 = df.loc[(df[\"Score\"] == 3)]\n",
    "df_4 = df.loc[(df[\"Score\"] == 4)]\n",
    "df_5 = df.loc[(df[\"Score\"] == 5)]\n",
    "\n",
    "# Splitting data into test and train folders with a 50/50 ratio\n",
    "df_1_train = df_1.sample(frac=sample_fraction, replace=False, random_state=1)\n",
    "df_1_test = df_1[~df_1.isin(df_1_train)].dropna(how='all')\n",
    "\n",
    "df_2_train = df_2.sample(frac=sample_fraction, replace=False, random_state=1)\n",
    "df_2_test = df_2[~df_2.isin(df_2_train)].dropna(how='all')\n",
    "\n",
    "df_3_train = df_3.sample(frac=sample_fraction, replace=False, random_state=1)\n",
    "df_3_test = df_3[~df_3.isin(df_3_train)].dropna(how='all')\n",
    "\n",
    "df_4_train = df_4.sample(frac=sample_fraction, replace=False, random_state=1)\n",
    "df_4_test = df_4[~df_4.isin(df_4_train)].dropna(how='all')\n",
    "\n",
    "df_5_train = df_5.sample(frac=sample_fraction, replace=False, random_state=1)\n",
    "df_5_test = df_5[~df_5.isin(df_5_train)].dropna(how='all')\n",
    "\n",
    "# Converting df values into txt files, putting them in the correct folders\n",
    "def save_reviews_to_folder(df, folder_path, score):\n",
    "    for index, row in df.iterrows():\n",
    "        path = f'{folder_path}/review{index}.txt'\n",
    "        with open(path, 'a') as f:\n",
    "            txt_in = row[\"Text\"]\n",
    "            f.write(txt_in)\n",
    "\n",
    "# Define folder paths\n",
    "train_folder_base = '../data/amazon_reviews/train'\n",
    "test_folder_base = '../data/amazon_reviews/test'\n",
    "\n",
    "# Save reviews to train and test folders\n",
    "save_reviews_to_folder(df_1_train, f'{train_folder_base}/1', 1)\n",
    "save_reviews_to_folder(df_1_test, f'{test_folder_base}/1', 1)\n",
    "\n",
    "save_reviews_to_folder(df_2_train, f'{train_folder_base}/2', 2)\n",
    "save_reviews_to_folder(df_2_test, f'{test_folder_base}/2', 2)\n",
    "\n",
    "save_reviews_to_folder(df_3_train, f'{train_folder_base}/3', 3)\n",
    "save_reviews_to_folder(df_3_test, f'{test_folder_base}/3', 3)\n",
    "\n",
    "save_reviews_to_folder(df_4_train, f'{train_folder_base}/4', 4)\n",
    "save_reviews_to_folder(df_4_test, f'{test_folder_base}/4', 4)\n",
    "\n",
    "save_reviews_to_folder(df_5_train, f'{train_folder_base}/5', 5)\n",
    "save_reviews_to_folder(df_5_test, f'{test_folder_base}/5', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n",
      "372\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick sanity check to make sure our reviews are \n",
    "# evenly distributed in our directory\n",
    "\n",
    "import os\n",
    "\n",
    "folder_path = '../data/amazon_reviews/train/4'\n",
    "\n",
    "# Get the list of items in the folder\n",
    "items = os.listdir(folder_path)\n",
    "\n",
    "# Print the number of items in the folder\n",
    "print(len(items))\n",
    "\n",
    "\n",
    "# repeat but with a different reviews folder\n",
    "folder_path = '../data/amazon_reviews/train/3'\n",
    "\n",
    "items = os.listdir(folder_path)\n",
    "\n",
    "print(len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1860 files belonging to 5 classes.\n",
      "Using 1488 files for training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1860 files belonging to 5 classes.\n",
      "Using 372 files for validation.\n",
      "Found 146985 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Now we will use the text_dataset_from_directory utility to create a labeled tf.data.Dataset.\n",
    "# Let's create a validation set using an 80:20 split of the training data by\n",
    "# using the validation_split argument below.\n",
    "\n",
    "# Note: When using the validation_split and subset arguments\n",
    "# make sure to either specify a random seed, or to pass shuffle=False, \n",
    "# so that the validation and training splits have no overlap.\n",
    "\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    '../data/amazon_reviews/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    labels='inferred',\n",
    "    subset='training',\n",
    "    seed=seed,\n",
    "    label_mode='int')\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    '../data/amazon_reviews/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    labels='inferred',\n",
    "    subset='validation',\n",
    "    seed=seed,\n",
    "    label_mode='int')\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    '../data/amazon_reviews/test',\n",
    "    batch_size=batch_size,\n",
    "    labels='inferred',\n",
    "    label_mode='int')\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(None,), dtype=tf.int32, name=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentative function for one hot encoding DONT NEED \n",
    "# def _map_func(text, labels):\n",
    "    # i=0\n",
    "    # labels_enc = tf.TensorArray(tf.float32, size=0, dynamic_size=True,clear_after_read=False)\n",
    "    # for label in labels:\n",
    "    #     if label.numpy()[i]==0:\n",
    "    #         label.numpy()[i] = 1\n",
    "    #     elif label.numpy()[i]==1:\n",
    "    #         label.numpy()[i] = 2\n",
    "    #     elif label.numpy()[i]==2:\n",
    "    #         label.numpy()[i] = 3\n",
    "    #     elif label.numpy()[i]==3:\n",
    "    #         label.numpy()[i] = 4\n",
    "    #     else: \n",
    "    #         label.numpy()[i] = 5\n",
    "        # label = tf.one_hot(label, 5, name='label', axis=-1)\n",
    "        # labels_enc.write(i, label)\n",
    "        # i+=1\n",
    "\n",
    "#     return text, labels\n",
    "    # pass\n",
    "# train_ds = train_ds.map(_map_func)\n",
    "# test_ds = test_ds.map(_map_func)\n",
    "# val_ds = val_ds.map(_map_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b\"I always by my Tabasco sauce from Amazon, the twelve oz. bottle for about $10.00.  I made a mistake while ordering and did not read the number of oz's on the bottle I was buying.  Because of the price, I just took it for granted it was the big bottle.  I was shocked when I saw the size of the bottle.  I can hardly believe I bought such a small bottle for that price.  In my opinion this item is grossly overpriced.\"\n",
      "Label : 0 (1)\n",
      "Review: b\"I absolutely love these bars, especially this flavor.  What i like the most is the natural flavor, and seriously low sugar count.  it's a great way to start the day, feel full and not bring a high sugar count into your system.  HIGHLY recommend!\"\n",
      "Label : 4 (5)\n",
      "Review: b'My kitty is 14 years old and she started throwing up alot this spring.  I switched to Natural Balance  and it was good for a month and then she started again.  So I tried the Longevity formula and so far have had great results.  First of all, she seemed to like the taste and the transition to a new food was a breeze.  She has been on this formula for a couple of months and has totally quit vomiting (except when she eats grass of course); she had lost some weight and seems more fit and trim.  I am sooo glad and highly recommend this formula.  I will see how she does over the winter when she is less active, but for now I am greatly pleased and so is she!'\n",
      "Label : 4 (5)\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(3):\n",
    "    print(f'Review: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32,), dtype=string, numpy=\n",
      "array([b\"I got a taste of these in a 2 oz bag and was told they weren't being made anymore. Imagine my surprise when I found them here!  I purchased the 5 oz bags and shared them with co-workers.<br /><br />From the first bite you get a nice smokey flavor and then the heat starts to kick in. After a few more chips you just can't stop crunching while beads of sweat pop out on your brow and your sinuses start clearning out.<br /><br />Love these chips!\",\n",
      "       b'Being a salt-free product is why I purchased this, but the chips are quite greasy.',\n",
      "       b\"The mix is great, but this price is more than twice that of Sam's Club. Doesn't make any sense.\",\n",
      "       b'I had high hopes for this product to work on my yard.  I used other remedies to get rid of the gophers and this was my last resort.  It did not work.  I may not have watered enough.',\n",
      "       b\"No Fear Super Energy is mostly made up of high fructose corn syrup - it's the #2 item on the ingredient list after carbonated water. Each 16oz can contains 66 grams of sugar (and 260 calories)! Yes, there are some other ingredients which may increase your energy, but most of the energy will undoubtedly be due to the HFCS.<br /><br />Each can is said to contain 2 servings, but since it's a carbonated beverage, it's not really going to keep well. The flavor is like carbonated cherry cough medicine.\",\n",
      "       b'These arrived in the middle of May and they have an expiration date of the first week of July. This means I have to eat a lot of these in the next few weeks! Just beware if you\\'re planning on stocking up for the future...the future will get here very soon with these!<br /><br />It\\'s also interesting that the box inside the Amazon box says \"For internal use only\". I\\'m not sure what that\\'s about!',\n",
      "       b\"I love the scent of this product. That's the good news.<br /><br />But in my old age, I have developed a horrible itchy reaction to certain hair products, namely WEN, which gave me an awful rash on my hairline (front and back) as well as my entire neck. So when I saw this product, I thought it would be a relief now that my itchy WEN rash is finally (after SIX months of healing) a thing of the past.<br /><br />Sad to say that I was wrong. I used Clear Scalp & Hair Beauty conditioner and shampoo  three times in a row and guess what happened?  Yup. Same sort of irritation and itching in all of the same places.<br /><br />There must be a very specific ingredient the makers of hair products are including in their respective cleansers and conditioners because the fact that this has happened AGAIN literally has me scratching my head in disbelief.<br /><br />Again, this conditioner has a lovely scent - but I will not spend my waking and sleeping hours scratching myself like a cat with fleas.  If you have any sensitivity to hair products, avoid this one.<br /><br />Boo. So goes this product the way of WEN. In the trash.\",\n",
      "       b\"I did not purchase mine from Amazon, but I did base my purchase on reviews I had seen on both Amazon & another site, which were all glowing.  So I thought it was fair to give another point of view.  I love popcorn more than any other food.  This is possibly the worst popcorn I have ever had.<br /><br />Normally, I use canola or peanut oil, fresh locally grown corn, & sea salt.  It is absolutely delicious with very few hulls or unpopped kernels.  It is much better (& better for you) than microwave or any other popping method using oil.<br /><br />This stuff was stale, & had way too much sodium.  The coconut oil mixture was disgusting. The taste was so bad that I threw the rest of it away after a few handfuls.  I had split the original batch in half, & gave 1/2 to my neighbor, who usually will eat anything.  Even she thought it was poor quality.  Now I'm stuck with 2 boxes of this junk.<br /><br />I gave it 1 star because every kernel did pop, and the cooking time was quick.  But I'm going back to, & sticking with, my old method.  I think it must be the locally grown corn (FRESH!) that spoiled me.\",\n",
      "       b\"This is a very pleasant green tea. The cinnamon is fairly dominant but counterbalanced quite well by the green tea flavor, even though Stash did use cinnamon oil.<br /><br />I was a little scared about the cinnamon after my unpleasant experience with Stash's Organic Chai Black & Green Tea, which was ruined by too much cinnamon oil.<br /><br />After I use up these six boxes I do not know if I will purchase anymore Stash tea.<br /><br />Stash needs to hold off on using the spice oils (especially the cinnamon), even though it isn't so offensive in this particular tea, they could still tone it down a bit more.\",\n",
      "       b'These bars were delicious. The nuts were great, the honey flavor came through, and the dark chocolate was spot on. The only problem was the texture - I ordered these in the middle of summer & the chocolate had melted & wandered around in the wrapper. I hope to have better results ordering these in winter.',\n",
      "       b\"I don't quite understand the proliferation of these squeeze bottle water additives when there are already so many energy and sports drinks on the market.  Trying to find the right mix that tastes right when you add it to water is not an easy task.  And to me it simply an unnecessary  chore when I can just buy a sports drink I like.<br /><br />This type of product is not for me.\",\n",
      "       b\"Love this coconut oil. Unrefined and a great light taste...great to cook with, bake with and even to add to smoothies! A much healthier alternative to canola, vegetable and other oils. It's all I use to cook with anymore. The only thing that could make it better would be if it was available on Subscribe & Save!\",\n",
      "       b\"I thought french roast would give me more flavor and smell, same thing, nothing special about this coffee, when opens up, it is flat and plain, no lust and didnt smell much, seems like it's been in the shelf for a long time. Price is good, that's the only good about it, otherwise it is just so-so.\",\n",
      "       b\"If you can stand the taste of Aspartame then you won't mind this product, the fruit punch flavor is ok except for that. Personally the aspartame taste was too strong and bitter for me to enjoy this drink. I wish that they could/would use sorbital in this product like they do in some of the bottled varieties.\",\n",
      "       b\"I love this beef jerky and so I looked everywhere for it, and had trouble finding it readily. So when I found this product on subscription on Amazon, I was more than thrilled to subscribe. That was two months ago. I've now cancelled my subscription. Why you ask? The unopened packages of the beef jerky were hard as a rock and were tasteless as a result. I don't know what Amazon does to receive the jerkys so cheap thereby giving the customers the 15% discount, but I'd rather pay more and get them off retail and enjoy every piece.. I am very disappointed at the result and won't re-subscribe again.\",\n",
      "       b\"Another non-diet food with artificial sweetener.  I tasted the bitterness of the sucralose, checked the label and sure enough,it was there at the end.  The first ingredient is SUGAR, why ruin it?  Skip this unless you're not on a diet, love sucralose, and not a chocolate lover.  Gross.\",\n",
      "       b'My kitty is 14 years old and she started throwing up alot this spring.  I switched to Natural Balance  and it was good for a month and then she started again.  So I tried the Longevity formula and so far have had great results.  First of all, she seemed to like the taste and the transition to a new food was a breeze.  She has been on this formula for a couple of months and has totally quit vomiting (except when she eats grass of course); she had lost some weight and seems more fit and trim.  I am sooo glad and highly recommend this formula.  I will see how she does over the winter when she is less active, but for now I am greatly pleased and so is she!',\n",
      "       b\"I like bold coffees and should have trusted the other reviewer who said it wasn't strong enough. This one has a watery finish to it and not very much flavor.  I'll be going back to Emeril's Jazz'd Up Decaf and trying the Coffee People's French Roast Decaf.  So far I've loved everything by the coffee people except the organic blend.  If you like strong coffee don't buy this but if you like a weaker coffee that you can see through you'd probably enjoy this one.\",\n",
      "       b\"I have ordered these cookies numerous times.  This last order though was horrible.  The cookies were all burnt little crisps.  I wrote to the company about this issue and got no response.  Won't be ordering these again.  Buy at your own risk.\",\n",
      "       b\"I didn't actually taste this, but I thought it had a strange smell.  My husband and my son love the dried mango we get at Costco so I thought I'd try this since it's organic.  Neither one of them will eat it....they think it has a strange taste.  Unfortunately, I might as well have thrown $20 in the trash can because that's where this mango ended up.\",\n",
      "       b\"I'm an avid coffee drinker.  I drink it black, cold, in a frap...  Illy happens to be one of my favorite brands for making lattes and cappuccinos, so I felt pretty confident that this drink would be good.  I was half right.  This may be one of the oddest things I've drunk yet.<br /><br />At the first sip, it tasted like lightly sweetened, fairly strong espresso, but there was something off.  I took another sip, trying to figure out what it was, but I couldn't quite put my finger on it.<br /><br />I decided to try pouring it into a glass to see if this alleviated the strangeness.  I took another sip and, yep, still strange.  It took me a while to realize that the strangeness was that it seemed almost like it was carbonated, yet it didn't fizz.  This made no sense to me, so I checked the label to find that potassium bicarbonate and potassium citrate are two of the ingredients.  Bingo.  My guess is these ingredients are conspiring to give this coffee a strange, difficult to describe aftertaste.<br /><br />Ultimately, this ruined the beverage for me.  Even though I liked the taste of the coffee and thought it had just the right level of sweetness, I couldn't get past that weird tang and vaguely fizzy feeling.  I won't be coming back for more of this.\",\n",
      "       b\"The first one I bit into was tough as leather and I had to rip it with my teeth. I made the mistake of biting it in half, and then I chewed, and chewed, and chewed. I couldn't get it to break down - it was like chewing gum! For the next one, I tried stripping off the outside, but that didn't make any difference. Still tough. I found they were easier to eat if I took tiny bites.<br /><br />That said, they were very tasty. But I don't like to fight my food, so I won't be buying this little stick any more.\",\n",
      "       b\"I was extremely disappionted when I opened my shipment of Enjoy Life On The Go Bars (Very Berry, Carmel Apple, and Cocoa Loco). The boxes are now a plain, generic design..you know, the kind all brands change to when they want to make their products look cheaper. I then compared the ingredients of the new shipment with the older boxes I still had at home. Guess what?! Cheaper ingredients! Next, I taste-tested, and let my 3 year old help. He wouldn't eat the new ones...and I can't blame him. They are oily and bland. I thought Enjoy Life was a brand that actually cared about using quality ingredients to make a good-tasting product. I was wrong. In summary, these are way too expensive to be made with low-quality, unpalatable ingredients.\",\n",
      "       b'Long time Keurig user and lifetime coffee drinker.  Breakfast Blend is for the most part a plain cup of coffee.  Light and smooth, but with a slight bitterness at the end.  I still prefer Donut Shop as a light everyday go to coffee choice, but wanted to try a box of Green Mountain BB.  Probably won\\'t purchase again.  No one in the house has stopped me and said, \"thats a great cup of coffee\".  It\\'s just plain and blah.  I\\'ll probably struggle to finish the box honestly.',\n",
      "       b\"I have to admit, I liked the watermelon strawberry flavor better than this one, kiwi berry, but this was just as flavorful and just as pleasant. Just the right amount of carbonation and sweetness, and I really loved the fact this has no corn syrup or refined sugar. The small can size is also very handy because it's easy to slip into a bag and it's exactly one serving size. As I mentioned with the other flavor, this drink doesn't have a weird aftertaste or leave a lingering flavor on the tongue--it's very clean and refreshing.\",\n",
      "       b'I am deeply disappointed that Quaker would add \"dinosaur eggs\" candy with partially hydrogenated oils to kids oatmeal.  Very shameful.  Save yourself big dollars and buy bulk organic oatmeal from any supermarket and add your own brown sugar or maple syrup.  Better for you and much, much cheaper.',\n",
      "       b\"I was hesitant about purchasing 24 boxes of jello in the same flavor, but this stuff is great!<br />We both love pina colada flavor and it's a quick, easy treat that we don't have to feel guilty about having.\",\n",
      "       b\"I've purchased 4 boxes of Bobo's Oat Bars in the Chocolate variety from Amazon in the past month.  An acquaintance had told me how delicious they were, so I took a chance and purchased my first box before I embarked on a trip, planning to take some along for the journey.  Unfortunately for my wallet, every person but one to whom I gave a bar loved these, so I sent some a box to enjoy.  Prior to my first purchase, I did not realize these are soft, sort of brownie like in texture, a little sweet in a molasses sort of way (no molasses in the recipe, though), crumbly, and with two servings in a single bar.  Fortunately, I placed a bar in a small resealable plastic bag for the airplane trip, so I was able to break off a little piece any time I felt hungry.  That bag was a saving grace because when a bar was gone, I replaced it with another and you won't believe how often I pulled out that little bag.  I ordered the first box as a gift after I put a little bite into my father's mouth while he was busy enjoying himself at a slot machine and I realized that he needed to keep these on hand for him and Mom to have on the days they go out.  For those who have few teeth, these are edible because of the soft texture, yet they provide an enjoyable flavor.  Hiker's took these along for a day trip, not worrying about the double serving per package, happily eating the entire bar.  As they put it, they felt like they had a brownie along for the trek, much more enjoyable than other bars they'd taken in the past.  One sister has enjoyed these on days when she's been unable to break for lunch at work, having them with coffee or tea and some deep breaths.  Without eggs, butter or flour, they are terrific, even though Earth Balance has never been my favorite flavor.  I'll admit that I was expecting a firmer bar with visible chocolate chips, but after the surprise of a crumbly moist baked good with a not very sweet cocoa flavor, I was hooked.  I will gladly try the other varieties when I can find them for a lower cost than over two dollars per bar.  A retail value of about thirty dollars is too high for a dozen bars, so I've deducted a star from this review.  The one friend who did not love her Bobo's Oat Bar thought it tasted like cardboard and she claimed to have forced it down at work.  She loves the SOYJOY bars which I do not care for, so I guess the world is balanced.\",\n",
      "       b'The Breakfast In Bed coffee from Wolfgang Puck is  a good medium roast coffee.  My husband, the regular coffee drinker of our house, likes it, but feels he would like the coffee more if it were a bit stronger.  The product arrived quite soon after ordering.',\n",
      "       b\"Impressed,as always,with Amazon's expedient delivery. I perused customer reviews before ordering and felt comfortable with my choice (I was going to get the Emeril Bourbon Street blend , which I love) as reviewers gave this a great rating. I find it strong but bitter as opposed to strong and rich. Would not recommend this blend to anyone. No negatives to Amazon but wonder who could find this a pleasurable coffee.\",\n",
      "       b'These bags had a lot of overcooked brown pieces. also felt very greasy. Had to keep wiping my fingers on a napkin.',\n",
      "       b\"I usually use Only Natural Pet Easy Raw dehydrated food, however I wanted to try Honest Kitchen's brand just to see how it compared, it looked & smelled like a pile of mush grass.  My dogs avoided it like the plague even my little girl who eats anything that is not nailed down.  I was panicking thinking I just wasted a huge chunk of money on something my fur kids won't eat when the Mail<br />Man rang and my delayed shipment of Only Natural brand freeze-dried chicken & beef patties came.  I was soooo happy!!!! I ripped open a bag of chicken, took a sniff, OMG it smelled so good!!!  I quickly crumbled the patty into the pile of green mush and my dogs scarfed it down in 5 minutes.  This food is so great & convenient.  Since it is expensive, I'm going to use it as a flavor enhancer in combination with other raw foods.  The Honest Kitchen food is very nutritious and easy to rehydrate.  However, it needs to be enhanced or combined with other ingredients such as meats (beef,turkey,chicken,buffalo, etc). I also add fish oil for shiney coats and sometimes add a raw egg as well.\"],\n",
      "      dtype=object)>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([3, 1, 1, 1, 1, 3, 1, 0, 2, 3, 0, 4, 0, 0, 2, 0, 4, 2, 0, 0, 1, 2,\n",
      "       1, 1, 4, 0, 4, 3, 3, 0, 2, 2], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "# looking at a batch \n",
    "for row in train_ds.take(1):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to 1\n",
      "Label 1 corresponds to 2\n"
     ]
    }
   ],
   "source": [
    "# Looking at what labels correspond to what ratings\n",
    "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "# Now we will load up a model with TensorFlow Hub\n",
    "# We will be using a small BERT to start with\n",
    "# to read about all the BERT model available click this link\n",
    "# https://colab.research.google.com/drive/1UytfDnUpCQTHK8BA8n__B4YCWm4gzIeW#scrollTo=dX8FtlpGJRE6\n",
    "\n",
    "\n",
    "\n",
    "#@title Choose a BERT model to fine-tune\n",
    "\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models.\n",
    "\n",
    "The preprocessing model must be the one referenced by the documentation of the BERT model.\n",
    "\n",
    "Note: We will load the preprocessing model into a hub.KerasLayer to compose your fine-tuned model. This is the preferred API to load a TF2-style SavedModel from TF Hub into a Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the preprocessing model on some text and see the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_type_ids', 'input_word_ids', 'input_mask']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [ 101 2023 2003 2107 2019 6429 4031  999  102    0    0    0]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "text_test = ['this is such an amazing product!']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, now you have the 3 outputs from the preprocessing that a BERT model would use (input_words_id, input_mask and input_type_ids).\n",
    "\n",
    "Some other important points:\n",
    "\n",
    "The input is truncated to 128 tokens. The number of tokens can be customized, and you can see more details on the Solve GLUE tasks using BERT on a TPU colab.\n",
    "The input_type_ids only have one value (0) because this is a single sentence input. For a multiple sentence input, it would have one number for each input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the BERT model\n",
    "\n",
    "Before putting BERT into our model, let's take a look at its outputs. We will load it from TF Hub and see the returned values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Pooled Outputs Shape:(1, 512)\n",
      "Pooled Outputs Values:[ 0.2560776   0.997757   -0.44676325  0.2663918   0.2193461   0.43699053\n",
      "  0.9671976  -0.9815668  -0.12048521 -0.98637974  0.09855995 -0.9820184 ]\n",
      "Sequence Outputs Shape:(1, 128, 512)\n",
      "Sequence Outputs Values:[[-0.37835765  0.79211026  0.17865962 ... -0.02951723  0.53888327\n",
      "  -0.10518374]\n",
      " [-0.22593209  0.5280527  -0.01349533 ...  0.7996965  -0.2248629\n",
      "   0.9144497 ]\n",
      " [-0.7240933   1.1752062  -0.8487482  ...  0.13510218 -0.39534006\n",
      "   0.7117282 ]\n",
      " ...\n",
      " [-0.05767796  0.0561219  -0.17037475 ...  0.46868354  0.7525876\n",
      "   0.5103831 ]\n",
      " [-0.10215646  0.1487371  -0.13012674 ...  0.43205702  1.0691458\n",
      "   0.272215  ]\n",
      " [-0.394735    0.60435593 -0.13972646 ... -0.07493115  1.1170473\n",
      "   0.25230798]]\n"
     ]
    }
   ],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT models return a map with 3 important keys: pooled_output, sequence_output, encoder_outputs:\n",
    "\n",
    "pooled_output represents each input sequence as a whole. The shape is [batch_size, H]. You can think of this as an embedding for the entire movie review.\n",
    "sequence_output represents each input token in the context. The shape is [batch_size, seq_length, H]. You can think of this as a contextual embedding for every token in the movie review.\n",
    "encoder_outputs are the intermediate activations of the L Transformer blocks. outputs[\"encoder_outputs\"][i] is a Tensor of shape [batch_size, seq_length, 1024] with the outputs of the i-th Transformer block, for 0 <= i < L. The last value of the list is equal to sequence_output.\n",
    "For the fine-tuning you are going to use the pooled_output array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your model\n",
    "You will create a very simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net =  tf.keras.layers.Dense(10, activation = 'relu')(net)\n",
    "  net = tf.keras.layers.Dense(5, activation='softmax', name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the model runs with the output of the preprocessing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.5414186  0.50925463 0.59455585 0.5494097  0.55374277]], shape=(1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is meaningless, of course, because the model has not been trained yet.\n",
    "\n",
    "Let's take a look at the model's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(classifier_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "You now have all the pieces to train a model, including the preprocessing module, BERT encoder, data, and classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "metrics = tf.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "For fine-tuning, let's use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as AdamW.\n",
    "\n",
    "For the learning rate (init_lr), you will use the same schedule as BERT pre-training: linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (num_warmup_steps). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the BERT model and training\n",
    "Using the classifier_model you created earlier, you can compile the model with the loss, metric and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: training time will vary depending on the complexity of the BERT model you have selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 407s 8s/step - loss: 1.6567 - sparse_categorical_accuracy: 0.1962 - val_loss: 1.6101 - val_sparse_categorical_accuracy: 0.1962\n",
      "Epoch 2/3\n",
      "47/47 [==============================] - 388s 8s/step - loss: 1.5637 - sparse_categorical_accuracy: 0.2923 - val_loss: 1.5505 - val_sparse_categorical_accuracy: 0.2930\n",
      "Epoch 3/3\n",
      "47/47 [==============================] - 400s 8s/step - loss: 1.4983 - sparse_categorical_accuracy: 0.3239 - val_loss: 1.5446 - val_sparse_categorical_accuracy: 0.2930\n"
     ]
    }
   ],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=train_ds,\n",
    "                               validation_data=val_ds,\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'amazon_reviews'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_my_examples(inputs, results):\n",
    "  result_for_printing = \\\n",
    "    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'for i in range(len(inputs))]\n",
    "  print(*result_for_printing, sep='\\n')\n",
    "  print()\n",
    "\n",
    "\n",
    "examples = [\n",
    "    'This is the best product I have ever bought in my life! SO amazing!!',  # this is the same sentence tried earlier\n",
    "    'The product was great!',\n",
    "    'The product was meh.',\n",
    "    'This was the absolute worst thing I have ever bought. I hate this product.',\n",
    "    'This product was ok',\n",
    "    'The product was so amazing! I love it!!!',\n",
    "    'This producr was so awful',\n",
    "    'I love this product more than my wife and kids'\n",
    "]\n",
    "\n",
    "reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
    "original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n",
    "\n",
    "print('Results from the saved model:')\n",
    "print_my_examples(examples, reloaded_results)\n",
    "print('Results from the model in memory:')\n",
    "print_my_examples(examples, original_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
