{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "# After doing some research, we will want to use TensorFlow, prob Keras (a deep\n",
    "# learning API written on top of TensorFlow, it's currently being used\n",
    "# in the LHC (Large Hadron Collider)).\n",
    "\n",
    "# We will be classifying text with BERT:\n",
    "# https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
    "\n",
    "# NOTE: as of 10/19/23 tensorflow will not run on windowns\n",
    "# I recomend running this through jupyterlab on a linux kernal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-text==2.13.*\n",
      "  Downloading tensorflow_text-2.13.0-cp38-cp38-macosx_10_9_x86_64.whl (6.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.5 MB 9.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow<2.14,>=2.13.0\n",
      "  Downloading tensorflow-2.13.1-cp38-cp38-macosx_10_15_x86_64.whl (216.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 216.2 MB 5.4 kB/s eta 0:00:011            | 51.9 MB 20.6 MB/s eta 0:00:0819.2 MB/s eta 0:00:08     |████████████████▊               | 112.7 MB 9.8 MB/s eta 0:00:11     |██████████████████▋             | 125.7 MB 12.6 MB/s eta 0:00:08████▌         | 151.7 MB 17.8 MB/s eta 0:00:04     |███████████████████████         | 154.7 MB 17.8 MB/s eta 0:00:04\n",
      "\u001b[?25hCollecting tensorflow-hub>=0.8.0\n",
      "  Downloading tensorflow_hub-0.15.0-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 6.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.15.0)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.10.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp38-cp38-macosx_10_14_x86_64.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 21.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions<4.6.0,>=3.6.6\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 19.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 6.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (52.0.0.post20210125)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[K     |████████████████████████████████| 440 kB 13.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.59.0-cp38-cp38-macosx_10_10_universal2.whl (9.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.6 MB 15.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-macosx_10_9_x86_64.whl (24.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.5 MB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.12.1)\n",
      "Collecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 14.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 4.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.24.4-cp37-abi3-macosx_10_9_universal2.whl (409 kB)\n",
      "\u001b[K     |████████████████████████████████| 409 kB 22.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy<=1.24.3,>=1.22\n",
      "  Downloading numpy-1.24.3-cp38-cp38-macosx_10_9_x86_64.whl (19.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.8 MB 21.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (23.1)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Applications/anaconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (0.36.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.25.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.8 MB 13.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.5-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 11.3 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 9.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Applications/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (3.4.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 5.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text==2.13.*) (4.0.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow-hub, tensorflow, tensorflow-text\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.10.0\n",
      "    Uninstalling importlib-metadata-3.10.0:\n",
      "      Successfully uninstalled importlib-metadata-3.10.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.7.1\n",
      "    Uninstalling typing-extensions-4.7.1:\n",
      "      Successfully uninstalled typing-extensions-4.7.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "islp 0.3.18 requires scikit-learn>=1.2, but you have scikit-learn 0.24.1 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.23.3 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.59.0 importlib-metadata-6.8.0 keras-2.13.1 libclang-16.0.6 markdown-3.5 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.24.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tensorflow-hub-0.15.0 tensorflow-io-gcs-filesystem-0.34.0 tensorflow-text-2.13.0 termcolor-2.3.0 typing-extensions-4.5.0\n"
     ]
    }
   ],
   "source": [
    "# A dependency of the preprocessing for BERT inputs\n",
    "!pip install -U \"tensorflow-text==2.13.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-models-official==2.13.*\n",
      "  Downloading tf_models_official-2.13.2-py2.py3-none-any.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-api-python-client>=1.6.7\n",
      "  Downloading google_api_python_client-2.104.0-py2.py3-none-any.whl (12.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.6 MB 19.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.10.1)\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting gin-config\n",
      "  Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Cython in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (0.29.23)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.8.1.78-cp37-abi3-macosx_10_16_x86_64.whl (54.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 54.7 MB 7.6 MB/s eta 0:00:011    |████                            | 6.8 MB 17.3 MB/s eta 0:00:037.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-text~=2.13.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.13.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.24.3)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 982 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (8.2.0)\n",
      "Collecting tf-slim>=1.1.0\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "\u001b[K     |████████████████████████████████| 352 kB 21.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.6.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (0.15.0)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.2.4)\n",
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.7-cp38-cp38-macosx_10_9_universal2.whl (168 kB)\n",
      "\u001b[K     |████████████████████████████████| 168 kB 8.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml>=6.0.0\n",
      "  Downloading PyYAML-6.0.1-cp38-cp38-macosx_10_9_x86_64.whl (191 kB)\n",
      "\u001b[K     |████████████████████████████████| 191 kB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacrebleu\n",
      "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-model-optimization>=0.4.1\n",
      "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
      "\u001b[K     |████████████████████████████████| 241 kB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.4.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (5.8.0)\n",
      "Collecting oauth2client\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (3.3.4)\n",
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 20.2 MB/s eta 0:00:01��█                   | 2.2 MB 20.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorflow~=2.13.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.13.1)\n",
      "Requirement already satisfied: six in /Applications/anaconda3/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.15.0)\n",
      "Collecting immutabledict\n",
      "  Downloading immutabledict-3.0.0-py3-none-any.whl (4.0 kB)\n",
      "Collecting kaggle>=1.3.9\n",
      "  Downloading kaggle-1.5.16.tar.gz (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 4.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting httplib2<1.dev0,>=0.15.0\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.23.3)\n",
      "Collecting uritemplate<5,>=3.0.1\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting google-auth-httplib2>=0.1.0\n",
      "  Downloading google_auth_httplib2-0.1.1-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5\n",
      "  Downloading google_api_core-2.12.0-py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 13.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
      "\u001b[K     |████████████████████████████████| 230 kB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (4.24.4)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.25.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (5.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Applications/anaconda3/lib/python3.8/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.4.7)\n",
      "Requirement already satisfied: certifi in /Applications/anaconda3/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (2020.12.5)\n",
      "Requirement already satisfied: python-dateutil in /Applications/anaconda3/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (2.8.1)\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (4.59.0)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-8.0.1-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: urllib3 in /Applications/anaconda3/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (1.26.4)\n",
      "Requirement already satisfied: bleach in /Applications/anaconda3/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (3.3.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Applications/anaconda3/lib/python3.8/site-packages (from pandas>=0.22.0->tf-models-official==2.13.*) (2021.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Applications/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (0.5.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (4.0.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.3.0)\n",
      "Requirement already satisfied: packaging in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (23.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (23.5.26)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (16.0.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.59.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (4.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.34.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (52.0.0.post20210125)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Applications/anaconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow~=2.13.0->tf-models-official==2.13.*) (0.36.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (0.7.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.5)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Applications/anaconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Applications/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.4.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.2.2)\n",
      "Collecting dm-tree~=0.1.1\n",
      "  Downloading dm_tree-0.1.8-cp38-cp38-macosx_10_9_x86_64.whl (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 27.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: webencodings in /Applications/anaconda3/lib/python3.8/site-packages (from bleach->kaggle>=1.3.9->tf-models-official==2.13.*) (0.5.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->tf-models-official==2.13.*) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->tf-models-official==2.13.*) (0.10.0)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: colorama in /Applications/anaconda3/lib/python3.8/site-packages (from sacrebleu->tf-models-official==2.13.*) (0.4.4)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: lxml in /Applications/anaconda3/lib/python3.8/site-packages (from sacrebleu->tf-models-official==2.13.*) (4.6.3)\n",
      "Requirement already satisfied: regex in /Applications/anaconda3/lib/python3.8/site-packages (from sacrebleu->tf-models-official==2.13.*) (2021.4.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Applications/anaconda3/lib/python3.8/site-packages (from seqeval->tf-models-official==2.13.*) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Applications/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.13.*) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.13.*) (2.1.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: importlib-resources in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (6.0.1)\n",
      "Requirement already satisfied: click in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (7.1.2)\n",
      "Collecting etils[enp,epath]>=0.9.0\n",
      "  Downloading etils-1.3.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: toml in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (0.10.2)\n",
      "Collecting array-record\n",
      "  Downloading array_record-0.4.0-py38-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 16.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5\n",
      "  Downloading protobuf-3.20.3-cp38-cp38-macosx_10_9_x86_64.whl (982 kB)\n",
      "\u001b[K     |████████████████████████████████| 982 kB 9.3 MB/s eta 0:00:0152 kB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: kaggle, seqeval, promise\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.16-py3-none-any.whl size=110687 sha256=7724f7a79166ab09df94fbeff9b883d41cee2cad8e22686bcb3e3b4ce6c4f11c\n",
      "  Stored in directory: /Users/sebastianvargas/Library/Caches/pip/wheels/5a/ab/50/e224f599a07faf6d398a8600796012da271b7e5e7f2a3ab2b8\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16170 sha256=762dadce05373039710cfb54a856d2d14f93678117b0f607ffc6e1d057579287\n",
      "  Stored in directory: /Users/sebastianvargas/Library/Caches/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=143e70aca5f139da867dc2d53c926761c5f9005305708450ef0ea4b2d79ffe9f\n",
      "  Stored in directory: /Users/sebastianvargas/Library/Caches/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built kaggle seqeval promise\n",
      "Installing collected packages: etils, protobuf, absl-py, text-unidecode, httplib2, googleapis-common-protos, uritemplate, tensorflow-metadata, tabulate, python-slugify, promise, portalocker, google-auth-httplib2, google-api-core, dm-tree, array-record, tf-slim, tensorflow-model-optimization, tensorflow-datasets, seqeval, sentencepiece, sacrebleu, pyyaml, pycocotools, py-cpuinfo, opencv-python-headless, oauth2client, kaggle, immutabledict, google-api-python-client, gin-config, tf-models-official\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.24.4\n",
      "    Uninstalling protobuf-4.24.4:\n",
      "      Successfully uninstalled protobuf-4.24.4\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 2.0.0\n",
      "    Uninstalling absl-py-2.0.0:\n",
      "      Successfully uninstalled absl-py-2.0.0\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 5.4.1\n",
      "    Uninstalling PyYAML-5.4.1:\n",
      "      Successfully uninstalled PyYAML-5.4.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\u001b[0m\n",
      "Successfully installed absl-py-1.4.0 array-record-0.4.0 dm-tree-0.1.8 etils-1.3.0 gin-config-0.5.0 google-api-core-2.12.0 google-api-python-client-2.104.0 google-auth-httplib2-0.1.1 googleapis-common-protos-1.61.0 httplib2-0.22.0 immutabledict-3.0.0 kaggle-1.5.16 oauth2client-4.1.3 opencv-python-headless-4.8.1.78 portalocker-2.8.2 promise-2.3 protobuf-3.20.3 py-cpuinfo-9.0.0 pycocotools-2.0.7 python-slugify-8.0.1 pyyaml-6.0.1 sacrebleu-2.3.1 sentencepiece-0.1.99 seqeval-1.2.2 tabulate-0.9.0 tensorflow-datasets-4.9.2 tensorflow-metadata-1.14.0 tensorflow-model-optimization-0.7.5 text-unidecode-1.3 tf-models-official-2.13.2 tf-slim-1.1.0 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "# Use use the AdamW optimizer from https://github.com/tensorflow/models.\n",
    "!pip install \"tf-models-official==2.13.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nesisary libraries\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp \n",
    "import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataset directory structure\n",
    "# This will make it easy to organize and accesss our data in our directory structure\n",
    "\n",
    "\n",
    "dataset_dir = '../data/amazon_reviews'\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Make it easy to access 'train' and 'test' directories inside the dataset directory\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "\n",
    "# This will build the 'train' and 'test' directories if they haven't been built yet\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# This will build the folders 1, 2, 3, 4, 5 inside both the train and test directories\n",
    "one_dir_train = os.path.join(train_dir, '1')\n",
    "two_dir_train = os.path.join(train_dir, '2')\n",
    "three_dir_train = os.path.join(train_dir, '3')\n",
    "four_dir_train = os.path.join(train_dir, '4')\n",
    "five_dir_train = os.path.join(train_dir, '5')\n",
    "os.makedirs(one_dir_train, exist_ok=True)\n",
    "os.makedirs(two_dir_train, exist_ok=True)\n",
    "os.makedirs(three_dir_train, exist_ok=True)\n",
    "os.makedirs(four_dir_train, exist_ok=True)\n",
    "os.makedirs(five_dir_train, exist_ok=True)\n",
    "one_dir_test = os.path.join(test_dir, '1')\n",
    "two_dir_test = os.path.join(test_dir, '2')\n",
    "three_dir_test = os.path.join(test_dir, '3')\n",
    "four_dir_test = os.path.join(test_dir, '4')\n",
    "five_dir_test = os.path.join(test_dir, '5')\n",
    "os.makedirs(one_dir_test, exist_ok=True)\n",
    "os.makedirs(two_dir_test, exist_ok=True)\n",
    "os.makedirs(three_dir_test, exist_ok=True)\n",
    "os.makedirs(four_dir_test, exist_ok=True)\n",
    "os.makedirs(five_dir_test, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text\n",
       "0      5  I have bought several of the Vitality canned d...\n",
       "1      1  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2      4  This is a confection that has been around a fe...\n",
       "3      2  If you are looking for the secret ingredient i...\n",
       "4      5  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading the dataset\n",
    "# In order to run this project drop in the .csv data set (called: \"Reviews\") into the \"data\" folder\n",
    "# You can find the data set here:\n",
    "# https://www.google.com/url?q=https://www.kaggle.com/datasets/arhamrumi/amazon-product-reviews&sa=D&source=docs&ust=1695764896933142&usg=AOvVaw1WeATDdUdlTItgNGGldkRF\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_extra_params = pd.read_csv(\"../data/Reviews.csv\")\n",
    "# Only use the Score and Text paramaters\n",
    "# (Score: 1-5 stars, Text: an amazon review)\n",
    "df = df_extra_params[[\"Score\", \"Text\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21320\n",
      "21320\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-5d76eaddb9ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_1_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/amazon_reviews/test/1/my_test.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mtxt_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/_bootlocale.py\u001b[0m in \u001b[0;36mgetpreferredencoding\u001b[0;34m(do_setlocale)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdo_setlocale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf8_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Copying the data into our directory structure\n",
    "# We will seperate the data into the train and test folders\n",
    "# Inside the train and test folders we have folders 1, 2, 3, 4, 5\n",
    "# This corresponds to the \"Score\" of the review\n",
    "\n",
    "# THIS IS A WORK IN PROGRESS\n",
    "\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Seperate all data into 5 dfs for scores 1-5 respectively\n",
    "df_1 = df.loc[(df[\"Score\"] == 1)]\n",
    "df_2 = df.loc[(df[\"Score\"] == 2)]\n",
    "df_3 = df.loc[(df[\"Score\"] == 3)]\n",
    "df_4 = df.loc[(df[\"Score\"] == 4)]\n",
    "df_5 = df.loc[(df[\"Score\"] == 5)]\n",
    "\n",
    "# Splitting data into test and train folders with 50/50 ratio\n",
    "df_1_train = df_1.sample(frac=0.5, replace=False, random_state=1)\n",
    "df_1_test = df_1[~df_1.isin(df_1_train)].dropna(how = 'all')\n",
    "\n",
    "df_2_train = df_2.sample(frac=0.5, replace=False, random_state=1)\n",
    "df_2_test = df_2[~df_2.isin(df_2_train)].dropna(how = 'all')\n",
    "\n",
    "df_3_train = df_3.sample(frac=0.5, replace=False, random_state=1)\n",
    "df_3_test = df_3[~df_3.isin(df_3_train)].dropna(how = 'all')\n",
    "\n",
    "df_4_train = df_4.sample(frac=0.5, replace=False, random_state=1)\n",
    "df_4_test = df_4[~df_4.isin(df_4_train)].dropna(how = 'all')\n",
    "\n",
    "df_5_train = df_5.sample(frac=0.5, replace=False, random_state=1)\n",
    "df_5_test = df_5[~df_5.isin(df_5_train)].dropna(how = 'all')\n",
    "\n",
    "df_2_train.head()\n",
    "print(len(df_3_test))\n",
    "print(len(df_3_train))\n",
    "\n",
    "for i in range(0, len(df_1_test)):\n",
    "    with open(f'test{i}.txt', 'w'):\n",
    "        pass\n",
    "\n",
    "for index, row in df_1_test.iterrows():\n",
    "    path = '../data/amazon_reviews/test/1/my_test.txt'\n",
    "    with open(path, 'a') as f:\n",
    "        txt_in = row[\"Text\"]\n",
    "        f.write(txt_in)\n",
    "\n",
    "# cwd = os.getcwd()\n",
    "# path = cwd + \"/test\"\n",
    "# df_1_test.to_csv(path)\n",
    "\n",
    "\n",
    "# for row in df:\n",
    "#     train_test_iterator += 1\n",
    "#     review = row['Text']\n",
    "#     if random.randint(1,10) > 1:\n",
    "#         if train_test_iterator%10 == 0:\n",
    "#             if df['Score'] == 1:\n",
    "#             shutil.copy(review_file, one_dir_train)\n",
    "#     else: \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory data/amazon_reviews/train",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-48214fda504a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m'data/amazon_reviews/train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/keras/src/utils/text_dataset.py\u001b[0m in \u001b[0;36mtext_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, batch_size, max_length, shuffle, seed, validation_split, subset, follow_links)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     file_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     raise errors.NotFoundError(\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory data/amazon_reviews/train"
     ]
    }
   ],
   "source": [
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "# batch_size = 32\n",
    "# seed = 42\n",
    "\n",
    "# raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "#     'data/amazon_reviews/train',\n",
    "#     batch_size=batch_size,\n",
    "#     validation_split=0.2,\n",
    "#     subset='training',\n",
    "#     seed=seed)\n",
    "\n",
    "# class_names = raw_train_ds.class_names\n",
    "# train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "#     'aclImdb/train',\n",
    "#     batch_size=batch_size,\n",
    "#     validation_split=0.2,\n",
    "#     subset='validation',\n",
    "#     seed=seed)\n",
    "\n",
    "# val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "#     'aclImdb/test',\n",
    "#     batch_size=batch_size)\n",
    "\n",
    "# test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
