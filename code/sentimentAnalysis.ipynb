{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "# After doing some research, we will want to use TensorFlow, prob Keras (a deep\n",
    "# learning API written on top of TensorFlow, it's currently being used\n",
    "# in the LHC (Large Hadron Collider)).\n",
    "\n",
    "# We will be classifying text with BERT:\n",
    "# https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
    "\n",
    "# NOTE: as of 10/19/23 tensorflow will not run on windowns\n",
    "# I recomend running this through jupyterlab on a linux kernal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Requirement already up-to-date: tensorflow-text==2.13.* in /home/owenkutzscher/.local/lib/python3.8/site-packages (2.13.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-hub>=0.8.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow-text==2.13.*) (0.15.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\" in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow-text==2.13.*) (2.13.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow-hub>=0.8.0->tensorflow-text==2.13.*) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.19.6 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow-hub>=0.8.0->tensorflow-text==2.13.*) (4.24.4)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\" in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (0.34.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio<2.0,>=1.24.3 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (1.59.0)\n",
      "Requirement already satisfied, skipping upgrade: flatbuffers>=23.1.21 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (23.5.26)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/lib/python3/dist-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (45.2.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions<4.6.0,>=3.6.6 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (4.5.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py>=2.9.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (3.10.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: libclang>=13.0.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (16.0.6)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (23.2)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.14,>=2.13.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (2.13.0)\n",
      "Requirement already satisfied, skipping upgrade: astunparse>=1.6.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: keras<2.14,>=2.13.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (2.13.1)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=1.0.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: gast<=0.4.0,>=0.2.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<2.14,>=2.13 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (2.13.0)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (3.5)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=1.0.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<3,>=1.6.3 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (2.23.3)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<1.1,>=0.5 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.8.0,>=0.7.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (2.31.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=4.4; python_version < \"3.10\" in /home/owenkutzscher/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (6.8.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=2.1.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (2.1.3)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (4.9)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<6.0,>=2.0.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (3.17.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (0.4.2)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.13.*) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "# A dependency of the preprocessing for BERT inputs\n",
    "!pip install -U \"tensorflow-text==2.13.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tf-models-official==2.13.* in /home/owenkutzscher/.local/lib/python3.8/site-packages (2.13.2)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (0.15.0)\n",
      "Requirement already satisfied: py-cpuinfo>=3.3.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (9.0.0)\n",
      "Requirement already satisfied: Cython in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (3.0.4)\n",
      "Requirement already satisfied: oauth2client in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (4.1.3)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.24.3)\n",
      "Requirement already satisfied: sacrebleu in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.3.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.10.1)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (6.0.1)\n",
      "Requirement already satisfied: kaggle>=1.3.9 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.5.16)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (5.9.6)\n",
      "Requirement already satisfied: pycocotools in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.0.7)\n",
      "Requirement already satisfied: tensorflow~=2.13.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.13.1)\n",
      "Requirement already satisfied: sentencepiece in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (0.1.99)\n",
      "Requirement already satisfied: Pillow in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (10.1.0)\n",
      "Requirement already satisfied: immutabledict in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (3.0.0)\n",
      "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (0.7.5)\n",
      "Requirement already satisfied: opencv-python-headless in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (4.8.1.78)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.104.0)\n",
      "Requirement already satisfied: seqeval in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.2.2)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.0.3)\n",
      "Requirement already satisfied: gin-config in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (0.5.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from tf-models-official==2.13.*) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-datasets in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (4.9.2)\n",
      "Requirement already satisfied: matplotlib in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (3.7.3)\n",
      "Requirement already satisfied: tensorflow-text~=2.13.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (2.13.0)\n",
      "Requirement already satisfied: tf-slim>=1.1.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tf-models-official==2.13.*) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow-hub>=0.6.0->tf-models-official==2.13.*) (4.24.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/lib/python3/dist-packages (from oauth2client->tf-models-official==2.13.*) (0.2.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/lib/python3/dist-packages (from oauth2client->tf-models-official==2.13.*) (0.4.2)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from oauth2client->tf-models-official==2.13.*) (4.9)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from oauth2client->tf-models-official==2.13.*) (0.22.0)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from sacrebleu->tf-models-official==2.13.*) (0.4.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from sacrebleu->tf-models-official==2.13.*) (0.9.0)\n",
      "Requirement already satisfied: portalocker in /home/owenkutzscher/.local/lib/python3.8/site-packages (from sacrebleu->tf-models-official==2.13.*) (2.8.2)\n",
      "Requirement already satisfied: lxml in /home/owenkutzscher/.local/lib/python3.8/site-packages (from sacrebleu->tf-models-official==2.13.*) (4.9.3)\n",
      "Requirement already satisfied: regex in /home/owenkutzscher/.local/lib/python3.8/site-packages (from sacrebleu->tf-models-official==2.13.*) (2023.10.3)\n",
      "Requirement already satisfied: python-dateutil in /home/owenkutzscher/.local/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (2.8.2)\n",
      "Requirement already satisfied: requests in /home/owenkutzscher/.local/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /home/owenkutzscher/.local/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (4.66.1)\n",
      "Requirement already satisfied: bleach in /home/owenkutzscher/.local/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (6.1.0)\n",
      "Requirement already satisfied: urllib3 in /usr/lib/python3/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (1.25.8)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (2019.11.28)\n",
      "Requirement already satisfied: python-slugify in /home/owenkutzscher/.local/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official==2.13.*) (8.0.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (45.2.0)\n",
      "Requirement already satisfied: packaging in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (23.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (4.5.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (23.5.26)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (16.0.6)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.4.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (2.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.59.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\" in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.34.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (3.10.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow~=2.13.0->tf-models-official==2.13.*) (0.2.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.13.*) (0.1.8)\n",
      "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.23.3)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (0.1.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (2.12.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.13.*) (4.1.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from seqeval->tf-models-official==2.13.*) (1.3.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from pandas>=0.22.0->tf-models-official==2.13.*) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from pandas>=0.22.0->tf-models-official==2.13.*) (2023.3.post1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from tensorflow-datasets->tf-models-official==2.13.*) (7.0)\n",
      "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (1.3.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (1.14.0)\n",
      "Requirement already satisfied: array-record in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (0.4.0)\n",
      "Requirement already satisfied: toml in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (0.10.2)\n",
      "Requirement already satisfied: promise in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (2.3)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official==2.13.*) (6.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from matplotlib->tf-models-official==2.13.*) (4.43.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from matplotlib->tf-models-official==2.13.*) (3.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from matplotlib->tf-models-official==2.13.*) (1.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from matplotlib->tf-models-official==2.13.*) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from matplotlib->tf-models-official==2.13.*) (0.12.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->kaggle>=1.3.9->tf-models-official==2.13.*) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from requests->kaggle>=1.3.9->tf-models-official==2.13.*) (3.3.0)\n",
      "Requirement already satisfied: webencodings in /home/owenkutzscher/.local/lib/python3.8/site-packages (from bleach->kaggle>=1.3.9->tf-models-official==2.13.*) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.13.*) (1.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (0.7.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.5)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.0.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (0.34.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (5.3.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.13.*) (1.61.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.13.*) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.13.*) (1.3.2)\n",
      "Requirement already satisfied: zipp; extra == \"epath\" in /home/owenkutzscher/.local/lib/python3.8/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tf-models-official==2.13.*) (3.17.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/owenkutzscher/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (6.8.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/owenkutzscher/.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (2.1.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official==2.13.*) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Use use the AdamW optimizer from https://github.com/tensorflow/models.\n",
    "!pip install \"tf-models-official==2.13.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 18:04:34.306804: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-03 18:04:35.172882: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-03 18:04:35.180764: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-03 18:04:37.797244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# import nesisary libraries\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text \n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import keras\n",
    "import keras.utils\n",
    "import keras.layers\n",
    "import keras.models\n",
    "import keras.optimizers\n",
    "from keras import utils as np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataset directory structure\n",
    "# This will make it easy to organize and accesss our data in our directory structure\n",
    "\n",
    "\n",
    "dataset_dir = '../data/amazon_reviews'\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Make it easy to access 'train' and 'test' directories inside the dataset directory\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "\n",
    "# This will build the 'train' and 'test' directories if they haven't been built yet\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# This will build the folders 1, 2, 3, 4, 5 inside both the train and test directories\n",
    "one_dir_train = os.path.join(train_dir, '1')\n",
    "two_dir_train = os.path.join(train_dir, '2')\n",
    "three_dir_train = os.path.join(train_dir, '3')\n",
    "four_dir_train = os.path.join(train_dir, '4')\n",
    "five_dir_train = os.path.join(train_dir, '5')\n",
    "os.makedirs(one_dir_train, exist_ok=True)\n",
    "os.makedirs(two_dir_train, exist_ok=True)\n",
    "os.makedirs(three_dir_train, exist_ok=True)\n",
    "os.makedirs(four_dir_train, exist_ok=True)\n",
    "os.makedirs(five_dir_train, exist_ok=True)\n",
    "one_dir_test = os.path.join(test_dir, '1')\n",
    "two_dir_test = os.path.join(test_dir, '2')\n",
    "three_dir_test = os.path.join(test_dir, '3')\n",
    "four_dir_test = os.path.join(test_dir, '4')\n",
    "five_dir_test = os.path.join(test_dir, '5')\n",
    "os.makedirs(one_dir_test, exist_ok=True)\n",
    "os.makedirs(two_dir_test, exist_ok=True)\n",
    "os.makedirs(three_dir_test, exist_ok=True)\n",
    "os.makedirs(four_dir_test, exist_ok=True)\n",
    "os.makedirs(five_dir_test, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Do not bother. They are only good for 4 months...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I bought this coffee because the cost is about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Come on AMZN - or someone - straighten out the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This review will make me sound really stupid, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Apparently this deal for people who are really...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text\n",
       "0      1  Do not bother. They are only good for 4 months...\n",
       "1      1  I bought this coffee because the cost is about...\n",
       "2      1  Come on AMZN - or someone - straighten out the...\n",
       "3      1  This review will make me sound really stupid, ...\n",
       "4      1  Apparently this deal for people who are really..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading the dataset\n",
    "# IMPORTANT!!!!\n",
    "# In order to run this project drop in the .csv data set (called: \"Reviews\") into the \"data\" folder\n",
    "# You can find the data set here:\n",
    "# https://www.google.com/url?q=https://www.kaggle.com/datasets/arhamrumi/amazon-product-reviews&sa=D&source=docs&ust=1695764896933142&usg=AOvVaw1WeATDdUdlTItgNGGldkRF\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df_extra_params = pd.read_csv(\"../data/Reviews.csv\")\n",
    "\n",
    "# Select only the \"Score\" and \"Text\" columns\n",
    "df_unbalanced = df_extra_params[[\"Score\", \"Text\"]]\n",
    "\n",
    "# Determine minimum number of reviews for any score\n",
    "min_reviews = df_unbalanced['Score'].value_counts().min()\n",
    "\n",
    "# Make sure each review score 1-5 has the same number of reviews\n",
    "df = df_unbalanced.groupby('Score').apply(lambda x: x.sample(min_reviews)).reset_index(drop=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the data into our directory structure\n",
    "# We will seperate the data into the train and test folders\n",
    "# Inside the train and test folders we have folders 1, 2, 3, 4, 5\n",
    "# This corresponds to the \"Score\" of the review\n",
    "# Split with 50/50 ratio, randomly divided \n",
    "# Also take note of the \"sample_fraction\" variable, this is used to\n",
    "# reduce the ammount of data we will feed to BERT. This will help us\n",
    "# reduce training time when expiramenting with BERT.\n",
    "\n",
    "\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Path to the base folder\n",
    "base_folder = '../data/amazon_reviews'\n",
    "\n",
    "# Delete existing reviews from train and test folders\n",
    "for score_folder in ['1', '2', '3', '4', '5']:\n",
    "    train_folder_path = f'{base_folder}/train/{score_folder}'\n",
    "    test_folder_path = f'{base_folder}/test/{score_folder}'\n",
    "\n",
    "    shutil.rmtree(train_folder_path, ignore_errors=True)\n",
    "    shutil.rmtree(test_folder_path, ignore_errors=True)\n",
    "\n",
    "    # Recreate the train and test folders\n",
    "    os.makedirs(train_folder_path)\n",
    "    os.makedirs(test_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "# Determine the fraction for sampling (1/40th of the entire dataset)\n",
    "# Tip: for extra speedy runtimes, make this 1/40\n",
    "sample_fraction = 1/20\n",
    "\n",
    "# Separate all data into 5 dfs for scores 1-5 respectively\n",
    "df_1 = df.loc[(df[\"Score\"] == 1)]\n",
    "df_2 = df.loc[(df[\"Score\"] == 2)]\n",
    "df_3 = df.loc[(df[\"Score\"] == 3)]\n",
    "df_4 = df.loc[(df[\"Score\"] == 4)]\n",
    "df_5 = df.loc[(df[\"Score\"] == 5)]\n",
    "\n",
    "# Splitting data into test and train folders with a 50/50 ratio\n",
    "df_1_train = df_1.sample(frac=sample_fraction, replace=False, random_state=1)\n",
    "df_1_test = df_1[~df_1.isin(df_1_train)].dropna(how='all')\n",
    "\n",
    "df_2_train = df_2.sample(frac=sample_fraction, replace=False, random_state=1)\n",
    "df_2_test = df_2[~df_2.isin(df_2_train)].dropna(how='all')\n",
    "\n",
    "df_3_train = df_3.sample(frac=sample_fraction, replace=False, random_state=1)\n",
    "df_3_test = df_3[~df_3.isin(df_3_train)].dropna(how='all')\n",
    "\n",
    "df_4_train = df_4.sample(frac=sample_fraction, replace=False, random_state=1)\n",
    "df_4_test = df_4[~df_4.isin(df_4_train)].dropna(how='all')\n",
    "\n",
    "df_5_train = df_5.sample(frac=sample_fraction, replace=False, random_state=1)\n",
    "df_5_test = df_5[~df_5.isin(df_5_train)].dropna(how='all')\n",
    "\n",
    "# Converting df values into txt files, putting them in the correct folders\n",
    "def save_reviews_to_folder(df, folder_path, score):\n",
    "    for index, row in df.iterrows():\n",
    "        path = f'{folder_path}/review{index}.txt'\n",
    "        with open(path, 'a') as f:\n",
    "            txt_in = row[\"Text\"]\n",
    "            f.write(txt_in)\n",
    "\n",
    "# Define folder paths\n",
    "train_folder_base = '../data/amazon_reviews/train'\n",
    "test_folder_base = '../data/amazon_reviews/test'\n",
    "\n",
    "# Save reviews to train and test folders\n",
    "save_reviews_to_folder(df_1_train, f'{train_folder_base}/1', 1)\n",
    "save_reviews_to_folder(df_1_test, f'{test_folder_base}/1', 1)\n",
    "\n",
    "save_reviews_to_folder(df_2_train, f'{train_folder_base}/2', 2)\n",
    "save_reviews_to_folder(df_2_test, f'{test_folder_base}/2', 2)\n",
    "\n",
    "save_reviews_to_folder(df_3_train, f'{train_folder_base}/3', 3)\n",
    "save_reviews_to_folder(df_3_test, f'{test_folder_base}/3', 3)\n",
    "\n",
    "save_reviews_to_folder(df_4_train, f'{train_folder_base}/4', 4)\n",
    "save_reviews_to_folder(df_4_test, f'{test_folder_base}/4', 4)\n",
    "\n",
    "save_reviews_to_folder(df_5_train, f'{train_folder_base}/5', 5)\n",
    "save_reviews_to_folder(df_5_test, f'{test_folder_base}/5', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1488\n",
      "1488\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick sanity check to make sure our reviews are \n",
    "# evenly distributed in our directory\n",
    "\n",
    "import os\n",
    "\n",
    "folder_path = '../data/amazon_reviews/train/4'\n",
    "\n",
    "# Get the list of items in the folder\n",
    "items = os.listdir(folder_path)\n",
    "\n",
    "# Print the number of items in the folder\n",
    "print(len(items))\n",
    "\n",
    "\n",
    "# repeat but with a different reviews folder\n",
    "folder_path = '../data/amazon_reviews/train/3'\n",
    "\n",
    "items = os.listdir(folder_path)\n",
    "\n",
    "print(len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7440 files belonging to 5 classes.\n",
      "Using 5952 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 18:05:22.098842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-03 18:05:22.134061: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7440 files belonging to 5 classes.\n",
      "Using 1488 files for validation.\n",
      "Found 141405 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Now we will use the text_dataset_from_directory utility to create a labeled tf.data.Dataset.\n",
    "# Let's create a validation set using an 80:20 split of the training data by\n",
    "# using the validation_split argument below.\n",
    "\n",
    "# Note: When using the validation_split and subset arguments\n",
    "# make sure to either specify a random seed, or to pass shuffle=False, \n",
    "# so that the validation and training splits have no overlap.\n",
    "\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    '../data/amazon_reviews/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    labels='inferred',\n",
    "    subset='training',\n",
    "    seed=seed,\n",
    "    label_mode='int')\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    '../data/amazon_reviews/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    labels='inferred',\n",
    "    subset='validation',\n",
    "    seed=seed,\n",
    "    label_mode='int')\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    '../data/amazon_reviews/test',\n",
    "    batch_size=batch_size,\n",
    "    labels='inferred',\n",
    "    label_mode='int')\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(None,), dtype=tf.int32, name=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentative function for one hot encoding NOT RELIABLE \n",
    "def _map_func(text, labels):\n",
    "    # i=0\n",
    "    # labels_enc = tf.TensorArray(tf.float32, size=0, dynamic_size=True,clear_after_read=False)\n",
    "    # for text_batch, label_batch in train_ds.take(1):\n",
    "    #     label = label_batch.numpy()[i]\n",
    "    #     if label==0:\n",
    "    #         label = 1\n",
    "    #     elif label==1:\n",
    "    #         label = 2\n",
    "    #     elif label==2:\n",
    "    #         label = 3\n",
    "    #     elif label==3:\n",
    "    #         label = 4\n",
    "    #     else: \n",
    "    #         label = 5\n",
    "    #     print(f'Label : {label} ({class_names[label]})')\n",
    "    # for label in labels:\n",
    "    #     if label.numpy()[i]==0:\n",
    "    #         label.numpy()[i] = 1\n",
    "    #     elif label.numpy()[i]==1:\n",
    "    #         label.numpy()[i] = 2\n",
    "    #     elif label.numpy()[i]==2:\n",
    "    #         label.numpy()[i] = 3\n",
    "    #     elif label.numpy()[i]==3:\n",
    "    #         label.numpy()[i] = 4\n",
    "    #     else: \n",
    "    #         label.numpy()[i] = 5\n",
    "        # label = tf.one_hot(label, 5, name='label', axis=-1)\n",
    "        # labels_enc.write(i, label)\n",
    "        # i+=1\n",
    "\n",
    "#     return text, labels\n",
    "    pass\n",
    "# train_ds = train_ds.map(_map_func)\n",
    "# test_ds = test_ds.map(_map_func)\n",
    "# val_ds = val_ds.map(_map_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b\"While I have liked the other products in this line, using this was a disaster. My scalp has been itchy, with signs of dandruff, and I thought since the other products were so helpful this would be good as well. Yikes.<br /><br />For starters, there are no directions on the jar so what to do? It's very greasy but I took a handful and rubbed it into my scalp and massaged. So far, so good. I left it on my scalp for the rest of the evening, to give it a chance to work. The horror started when I tried to wash it out. After shampooing it three times, I gave up and just let it dry. The next morning I was a mass of greasy-looking curls. I'll be washing my hair every day until I get rid of this, but I certainly won't try it again.\"\n",
      "Label : 0 (1)\n",
      "Review: b'Good Earl Grey Tea but nothing about it was special. The \"double bergamot\" went unoticed.'\n",
      "Label : 2 (3)\n",
      "Review: b'Great taste with no saturated or trans fat.  A little high priced though, but well worth it for controlling cholesterol.'\n",
      "Label : 3 (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 18:05:43.767871: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(3):\n",
    "    print(f'Review: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32,), dtype=string, numpy=\n",
      "array([b'I wanted to really like this cereal.  We are big into old fashioned oatmeal for breakfast, as well as cooking with oats, in cookies and breads.  A ready-to-serve oat cereal, with enough actual oats in it to count, would be nice.  Unfortunately, we are underwhelmed by this product.  It has an unappetizing rather gray color.  The texture is a bit \"off\".  And the taste is bland... VERY bland.  The cinnamon is almost unrecognizable.  For the amount of sugar in it, it\\'s not very sweet tasting.  In general, we don\\'t really care for it much.  Sounds good, but we\\'ll stick to oatmeal....',\n",
      "       b'This was the first kind my family and I bought.  Lucky for me I didn\\'t let it stop me from trying the golden flax flavor.  OMG - I eat it for breakfast, lunch, snack - it\\'s the best way to \"cheat\" when you get those after hour urges.  It\\'s sprouted grains and very nutritious with a good amount of protein.  I have it after my superfood/fruit smoothie in the morning and have a lightness about me all day that I haven\\'t had in a while.  It makes it easy for me to eat 5 x\\'s a day and make healthy lunch choices at my sedentary job where there is free notsohealthy food offered all the time.  Even my 12 y/o daughter who hates lentils loved this and was VERY surprised when I told her what she was eating, lol.  We both hated the almond and love the golden flax but everyone\\'s taste buds are different.  FYI, the texture of the golden flax reminds me of grapenuts cereal. Can\\'t remember much about the almond except that we hated it and the full box is still in my kitchen and we are currently working on box # 2 of the golden flax.  Very happy and appreciative to find something that is so healthy taste so good.  Hope this helps!<br /><br />Love & Light,<br /><br />Angel',\n",
      "       b'Good stuff. I despise the taste of processed sweetners and I was hesitant about what they would taste like, but I was not disappointed. They are very good. Highly recommended.',\n",
      "       b\"Starbucks Vanilla Flavored Natural Fusions Ground Coffee features Arabica beans from Latin America and all natural flavors.  I first brewed this in an automatic drip coffee maker and awaited the results.  I was pleasantly surprised that the vanilla wasn't overpowering.  The results were typically Starbucks, strong but then a not so typical lack of bitterness with a hit of vanilla flavor. Excellent.<br /><br />I'll be the first to admit that I like flavored coffees but do not prefer Starbucks coffee.  To me, Starbucks is strong but too bitter.  This product, however, has the good strong blend without the bitterness and the bonus of Vanilla flavor.  I think this would be good for iced coffee drinks as well as homemade vanilla mochas and lattes.  I recommend this to Starbucks fans looking for a change and people who like strong, flavored coffee.\",\n",
      "       b\"I thought I would go back to my grandparents heritage,all four born in Ireland, and try some Irish Tea.  I recently had a heart problem and my cardiologist recommended that I significantly decreae my caffeine intake.  I used to drink several glasses of iced tea every day. Strangely enough tea has at least as much caffeine as coffee.  Green tea is definately out as an alternative to coffee or other beverages; but black tea is fine.I picked up some store brand black tea bags (Earl Grey, English Breakfast, Chai)to make iced tea and I was quite pleased with the heartiness of the flavor.  I then researched the matter further and came accross the Taylor's Decaffeinated Irish Breakfast Tea.  I was quite disappointed with the results.  I like strong tea, and must use at least 14 tea bags to get the same results as 10 tea bags of the store brand.  As you might guess the store brand ia a lot less expensive.  I will use up the 6 boxes that conisted of my order, and then buy the store brand again.\",\n",
      "       b'I love cappuccinos, but I get more of a cream and sugar taste from this than a coffee taste. That cream and sugar taste is pretty strong, reminds a little of sweetened condensed milk. The drink also has a powdery/chalky finish, probably due to the cocoa powder. I also noticed a metallic aftertaste. Mostly, just a lot of not very appealing flavors and textures coming together into a sugar bomb that was not very pleasant. Taste varies, but I would not drink it again.',\n",
      "       b\"This product is excellent but you have to carefully watch what you are paying for.  Some sellers are selling 6 bars for almost the same price as 12 and if you're not paying attention you could get caught.\",\n",
      "       b'Lipton soup is a good soup but the boxes were very small and the soup was not that great tasting. i would not recommend it.',\n",
      "       b\"For some reason I ordered far too many of these capers. It may have been the result of a confusing ad. However, I washed the salt off a few of them to get an idea of what they tasted like. I don't know if I rec'd a bad bag full, but I could find absolutely no taste in these capers. If you are set on ordering some, do not order too many.\",\n",
      "       b'This k-cup actually smelled up the office. And it tastes as good as it smells. I found it to be very flavorful.',\n",
      "       b\"My 6 month old kittens won't touch Wellness Kitten canned food.  I'm sure it's good for them but I can't get them to eat it.  I've tried on 2 separate occasions without any success.  One of my kittens is very finicky but the other one eats almost anything so I've given up on this brand.\",\n",
      "       b'The Bumble Bar tastes good but the sesame and flax seed tend to get stuck in your teeth.',\n",
      "       b'I discovered Popchips when I was on a trip to Vegas.  I doubted I could find them when I got home and I was almost right.  I found a couple of stores that had them but only one flavor and the price was exhorbitant.  I was thrilled to find them on Amazon.  They are low fat, low calorie, and still delicious.  The single serve bags are ideal for lunch or to throw in the car when on a short trip.',\n",
      "       b'My girlfriend and I just tried this eggplant, and we both agree that is is amazingly good.  She is Russian and said it is even better than anything her mother made when she was younger.  Just looking at the ingredients I\\'m shocked.<br /><br />\"Eggplant, Bell Pepper, White Wine Vinegar, Water, Corn Oil, Parsley, Dill, Sugar, Salt, Garlic, Black Pepper, Bay Leaf\"<br /><br />I would definitely recommend!',\n",
      "       b'Timely delivery of product.  Product description on web site indicates 4lb shipping weight!  Product only weighs 14 ounces.  Last I checked that is less than one pound.  Pricing is nearly four times that of comparable product of delivered size being offered by other vendors online.  I will shop elsewhere next time.',\n",
      "       b\"I bought these because they were all natural.  I've been trying to avoid eating foods containing artificial dyes and these were a great replacement for the popular candy that has dyes.  I find them to be a little less sweet but actually prefer that.  The candy shells are quite brittle and most of the candies came with cracked shells.  i.e. The bottom of the bag had quite a bit of chipped candy coating.  This didn't really bother me as I like these for the taste, not their appearance, but I could see how others may be turned off by this.  I'd buy these again.\",\n",
      "       b'Great taste with no saturated or trans fat.  A little high priced though, but well worth it for controlling cholesterol.',\n",
      "       b'My family loves poppy seed bread and I must say that these poppy seeds have the best flavor and they are so fresh. I keep one package in the refrigerator to use and store the rest in the freezer for later. I originally bought them to save money. The little jars you buy in the store are extremely expensive and you only get two loaves of bread out of them. I would definitly recommend buying these.',\n",
      "       b'If you prefer the taste of coconut water, then this item is not for you. However if you are looking for a substitute for a hydrating drink then this is a good alternative.<br /><br />It takes some experimentation to come up with the amount that suits your taste.<br />I prefer 3 scoops with a pinch of sugar although it doesnt come close to coconut water taste.<br />The downside of using more than one scoop is the salty aftertaste of the residue.<br /><br />2 stars since its not close to taste of natural coconut water.<br />3 stars as an alternative to hydrating drink.<br /><br />a single serving size of 1tbsp contains the following:<br />40  Calories<br />40mg  Sodium<br />326mg Potassium - this is 10% of the daily value<br />8g  Sugar<br /><br />3%  Calcium<br />6%  Manganese<br />15%  Magnesium<br />100% Vitamin C',\n",
      "       b\"We have purchased these dried apples several times now.  For me they're a great afternoon snack that satisfies my hunger but also my sweet tooth - they keep me from reaching for chocolate or other less healthy alternatives.  We'll continue to buy them.<br /><br />I would recommend that you keep them in a dry place and in a container that seals.  We live in a humid climate and had to discard some because they became spongy and gross.\",\n",
      "       b'I didn\\'t care for these coffee pods. In fact, they are the reason that I abandoned using a coffee maker and switched to a French Press and freshly roasted self-ground coffee from a local merchant.<br /><br />...and when I say \"freshly roasted,\" I mean that they roast the beans while I\\'m standing there.',\n",
      "       b'This is a great alternative to ketchup sweetened with high fructose corn syrup.  It does taste slightly different but not enough to cause me to not buy the product.  I would prefer this over not having ketchup on things like fries or hot dogs any day.',\n",
      "       b'I compared several packs of bay leaves to come to the conclusion that this was a good deal.  Unfortunately the description was written \"Ajika Bay Leaves, 16.4-Ounce\".  This is not a 16oz bag, but 16 leaves with a weight of 0.4 ounces.  Not Happy.',\n",
      "       b'I\\'ll keep it simple: I couldn\\'t taste any difference between Kraft\\'s \"Veggie Pasta\" Mac & Cheese and their regular Mac & Cheese. It\\'s yummy, not as rich (or calorie/fat-laden) as homemade, and reminiscent of many people\\'s dinner-or-lunch-at-home-when-parents-are-out and/or college days. The fact that this new option contains \"1/2 serving of vegetables per cup*\" is good... I guess. I do, however, find this addition of \"vegetables\" a bit unsettling. It\\'s great that kids (and adults) are getting \"extra\" vegetables, but I\\'ve got a few questions:<br /><br />* How exactly are these vegetables incorporated into the Mac & Cheese?  Are they powdered? Mushed and blended? Does the process that incorporates them into the M&C completely (or mostly) destroy the vegetables\\' nutritional value???<br /><br />* Does the addition of these vegetables - and marketing/pushing of said addition - mean that parents are going to stop worrying so much about making their kids eat actual vegetables, since they figure they\\'re getting \"something\" in their Mac & Cheese? (This scares me) Similarly, is this going to undermine the process of \"assisting\" kids in developing a taste for actual vegetables?<br /><br />I fear the answer to these questions is not great, and for those reasons, if the price of the new \"veggies added\" M&C was the same as the regular version (or only a few cents higher), I\\'d probably buy it (both for my nephews and myself). As soon as I placed it in my cart, however, I\\'d pretend that it wasn\\'t \"veggie-tabulous\" and still make sure that they (and I) had a FULL serving of a green vegetable on their (my) plate, alongside the yummy, bright-as-a-safety-cone Mac & Cheese.',\n",
      "       b\"I love my morning cup of coffee, but cannot have caffeine.  Finding a decent decaf coffee is difficult.  I am not a $tarbucks fan... I don't like bitter, acidic coffee.  The Lavazza Gran Filtro decaf whole bean coffee is a high quality medium roast coffee.  It is well balanced and has a great aroma.\",\n",
      "       b'Love the other flavors...this one is horrible.  And I cannot return...oops.  This flavor is almost a chemical one, does not taste anything like cheddar and my kids thought the same!',\n",
      "       b'This Dogswell food is one heck of a good buy for my pups, and they like it just fine. Trouble is, what used to look like \"stew\" (chunks of meaty things andt recognizable vegetables) now looks like barf. It must taste the same to the dogs, though, so no complaints have been registered. This is good, since I recently received a 4 case order. It seems to be the same high quality product that Dogswell has been producing before. Just looks different, that\\'s all. Must have been a corporate decision...I\\'ll keep on buying it, though I wish they would go back to the old formula.',\n",
      "       b'Since I have been known to burn water, my spouse took the reigns on preparing this dinner for our family. She advised me this was very to make...It only took her 3 minutes to do the food preparation and 15 minutes for the actual cooking/seasoning/serving.<br /><br />I\\'m the family taster, so I got first dip - and this tasted very cheesy, very creamy...that was because of the Velveeta cheese packet and not the powdered stuff. I did enjoy the additional seasonings, and I was prepared for a hearty meal. However, once the dehydrated broccoli package was added to the mix, I could barely see any of the broccoli in the completed meal. I\\'ve uploaded a picture in \"Customer Images\" to illustrate this.<br /><br />The preparation was quick, it was delicious, and it was a very quick 1-skillet cleanup. Each family member enjoyed this meal (since we\\'re partial to cheese), but I was expecting to see and taste a bit more broccoli.<br /><br />This piqued our interest and we will probably look for other flavors/variations of the Cheesy Skillet meals, but I\\'d suggest adding your own broccoli if that\\'s what you crave.',\n",
      "       b'Ordered this for my parents at christmas time and they had me order it for them again they enjoyed it so much. Every flavor and brand has a lot to offer.',\n",
      "       b\"For people avoiding sugar, Murray Sugar Free Cookies Oatmeal, are good enough! This whole thing about sugar free products got me intrigued and confused at the same time. Some products that advertised as being sugar free have a lot of other sources of sugar, like alcohol, that are high. The feeling I get with Murray's is that they are reliable in their claims. I see them in all sorts of reputable publications and that gives me encouragement regarding their advertisement. As to the taste I can say they are pretty good tasting. Have tried their other ones and this one and the chocolate are the ones I favor. Who am I? Hungry hungry all the time for sweets but I realize not all sugar free items are created equal. For now a decent 4 stars rating!\",\n",
      "       b'Works great for the Social Hour at the church.  No more sour milk!  Or forgotten creamer purchase!  Coffee drinkers like it.  And keeps coffee table tidy -- no messy milk spills/drips.  Great for storage -- makes purchase easy to have auto-delivery every 3 months.',\n",
      "       b'Great price for a great product.  Now we have enough oatmeal for several months.'],\n",
      "      dtype=object)>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([2, 0, 3, 3, 0, 1, 0, 0, 1, 3, 1, 2, 4, 4, 0, 3, 3, 4, 1, 4, 1, 3,\n",
      "       0, 3, 3, 0, 3, 3, 4, 3, 4, 3], dtype=int32)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 18:05:43.825446: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for row in train_ds.take(1):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to 1\n",
      "Label 1 corresponds to 2\n"
     ]
    }
   ],
   "source": [
    "# Looking at what labels correspond to what ratings\n",
    "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "# Now we will load up a model with TensorFlow Hub\n",
    "# We will be using a small BERT to start with\n",
    "# to read about all the BERT model available click this link\n",
    "# https://colab.research.google.com/drive/1UytfDnUpCQTHK8BA8n__B4YCWm4gzIeW#scrollTo=dX8FtlpGJRE6\n",
    "\n",
    "\n",
    "\n",
    "#@title Choose a BERT model to fine-tune\n",
    "\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models.\n",
    "\n",
    "The preprocessing model must be the one referenced by the documentation of the BERT model.\n",
    "\n",
    "Note: We will load the preprocessing model into a hub.KerasLayer to compose your fine-tuned model. This is the preferred API to load a TF2-style SavedModel from TF Hub into a Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the preprocessing model on some text and see the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_type_ids', 'input_mask', 'input_word_ids']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [ 101 2023 2003 2107 2019 6429 4031  999  102    0    0    0]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "text_test = ['this is such an amazing product!']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, now you have the 3 outputs from the preprocessing that a BERT model would use (input_words_id, input_mask and input_type_ids).\n",
    "\n",
    "Some other important points:\n",
    "\n",
    "The input is truncated to 128 tokens. The number of tokens can be customized, and you can see more details on the Solve GLUE tasks using BERT on a TPU colab.\n",
    "The input_type_ids only have one value (0) because this is a single sentence input. For a multiple sentence input, it would have one number for each input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the BERT model\n",
    "\n",
    "Before putting BERT into our model, let's take a look at its outputs. We will load it from TF Hub and see the returned values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 18:05:49.362924: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 62509056 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Pooled Outputs Shape:(1, 512)\n",
      "Pooled Outputs Values:[ 0.25607762  0.99775714 -0.44676334  0.26639184  0.21934545  0.4369908\n",
      "  0.9671976  -0.9815667  -0.12048458 -0.9863795   0.09855987 -0.9820184 ]\n",
      "Sequence Outputs Shape:(1, 128, 512)\n",
      "Sequence Outputs Values:[[-0.3783573   0.7921103   0.1786601  ... -0.02951667  0.538883\n",
      "  -0.10518394]\n",
      " [-0.22593099  0.5280534  -0.01349462 ...  0.79969656 -0.22486237\n",
      "   0.91444945]\n",
      " [-0.72409266  1.1752067  -0.8487483  ...  0.13510157 -0.39533997\n",
      "   0.7117271 ]\n",
      " ...\n",
      " [-0.05767777  0.0561218  -0.1703752  ...  0.46868443  0.7525872\n",
      "   0.51038325]\n",
      " [-0.1021568   0.14873698 -0.13012728 ...  0.43205714  1.0691454\n",
      "   0.2722145 ]\n",
      " [-0.39473408  0.60435665 -0.13972668 ... -0.07493064  1.1170473\n",
      "   0.2523077 ]]\n"
     ]
    }
   ],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT models return a map with 3 important keys: pooled_output, sequence_output, encoder_outputs:\n",
    "\n",
    "pooled_output represents each input sequence as a whole. The shape is [batch_size, H]. You can think of this as an embedding for the entire movie review.\n",
    "sequence_output represents each input token in the context. The shape is [batch_size, seq_length, H]. You can think of this as a contextual embedding for every token in the movie review.\n",
    "encoder_outputs are the intermediate activations of the L Transformer blocks. outputs[\"encoder_outputs\"][i] is a Tensor of shape [batch_size, seq_length, 1024] with the outputs of the i-th Transformer block, for 0 <= i < L. The last value of the list is equal to sequence_output.\n",
    "For the fine-tuning you are going to use the pooled_output array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your model\n",
    "You will create a very simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net =  tf.keras.layers.Dense(15, activation = 'relu')(net)\n",
    "  net = tf.keras.layers.Dense(5, activation='softmax', name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the model runs with the output of the preprocessing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 18:05:57.024328: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 62509056 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.51439446 0.6772151  0.531928   0.5114952  0.5068818 ]], shape=(1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is meaningless, of course, because the model has not been trained yet.\n",
    "\n",
    "Let's take a look at the model's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(classifier_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "You now have all the pieces to train a model, including the preprocessing module, BERT encoder, data, and classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "metrics = tf.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "For fine-tuning, let's use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as AdamW.\n",
    "\n",
    "For the learning rate (init_lr), you will use the same schedule as BERT pre-training: linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (num_warmup_steps). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the BERT model and training\n",
    "Using the classifier_model you created earlier, you can compile the model with the loss, metric and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: training time will vary depending on the complexity of the BERT model you have selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 18:06:02.390489: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 62509056 exceeds 10% of free system memory.\n",
      "2023-12-03 18:06:02.653054: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 62509056 exceeds 10% of free system memory.\n",
      "2023-12-03 18:06:07.430941: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 62509056 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=train_ds,\n",
    "                               validation_data=val_ds,\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'tensorFlow_amazon'\n",
    "folder_name = 'models'\n",
    "saved_model_path = os.path.join(folder_name, '{}_bert'.format(dataset_name.replace('/', '_')))\n",
    "\n",
    "# Ensure the folder exists\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  41/4594 [..............................] - ETA: 3:50:43 - loss: 0.0000e+00 - categorical_accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-9610144f1591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Loss: {loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Accuracy: {accuracy}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2198\u001b[0m                         ):\n\u001b[1;32m   2199\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2200\u001b[0;31m                             logs = test_function_runner.run_step(\n\u001b[0m\u001b[1;32m   2201\u001b[0m                                 \u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m                                 \u001b[0mdata_handler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   3998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3999\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4000\u001b[0;31m         \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4001\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4002\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    862\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m       (concrete_function,\n\u001b[1;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1348\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'amazon_reviews'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from the saved model:\n",
      "input: This is the best product I have ever bought in my life! SO amazing!! : score: 0.515502\n",
      "input: The product was great!         : score: 0.538779\n",
      "input: The product was meh.           : score: 0.605659\n",
      "input: This was the absolute worst thing I have ever bought. I hate this product. : score: 0.616166\n",
      "input: The product was so amazing! I love it!!! : score: 0.516273\n",
      "\n",
      "Results from the model in memory:\n",
      "input: This is the best product I have ever bought in my life! SO amazing!! : score: 0.515502\n",
      "input: The product was great!         : score: 0.538779\n",
      "input: The product was meh.           : score: 0.605659\n",
      "input: This was the absolute worst thing I have ever bought. I hate this product. : score: 0.616166\n",
      "input: The product was so amazing! I love it!!! : score: 0.516273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_my_examples(inputs, results):\n",
    "  result_for_printing = \\\n",
    "    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'for i in range(len(inputs))]\n",
    "  print(*result_for_printing, sep='\\n')\n",
    "  print()\n",
    "\n",
    "\n",
    "examples = [\n",
    "    'This is the best product I have ever bought in my life! SO amazing!!',  # this is the same sentence tried earlier\n",
    "    'The product was great!',\n",
    "    'The product was meh.',\n",
    "    'This was the absolute worst thing I have ever bought. I hate this product.',\n",
    "    'The product was so amazing! I love it!!!'\n",
    "]\n",
    "\n",
    "reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
    "original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n",
    "\n",
    "print('Results from the saved model:')\n",
    "print_my_examples(examples, reloaded_results)\n",
    "print('Results from the model in memory:')\n",
    "print_my_examples(examples, original_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
